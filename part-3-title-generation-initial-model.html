<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  3. Title Generation - Initial Model | Practical Machine Learning Adventures
</title>
  <link rel="canonical" href="https://benhoyle.github.io/part-3-title-generation-initial-model.html">


  <link rel="stylesheet" href="https://benhoyle.github.io/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.9/css/all.css" integrity="sha384-5SOiIsAziJl6AWe0HWRKTXlfcSHKmYV4RBF18PPJ173Kzn7jzMyFuTtk8JA7QQG1" crossorigin="anonymous">
  <link rel="stylesheet" href="https://benhoyle.github.io/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://benhoyle.github.io/theme/css/theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://benhoyle.github.io/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://benhoyle.github.io/feeds/title-generation.atom.xml">  
  <meta name="description" content="This post looks at implementing an initial model.">
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o);
      a.async = 1;
      a.src = g;
      m = s.getElementsByTagName(o)[0];
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-96290248-2', 'auto');
    ga('send', 'pageview');
  </script>


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="https://benhoyle.github.io/">Practical Machine Learning Adventures</a></h1>
      <p class="text-muted">A selection of machine learning projects</p>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
  <article class="article">
    <div class="content">
      <h1>3. Title Generation - Initial Model</h1>
<p>Given our analysis in the previous post we will now construct our model.</p>
<hr>
<h2>Load and Tokenize Data</h2>
<div class="highlight"><pre><span></span><span class="c1"># Set parameters</span>
<span class="n">num_decoder_tokens</span> <span class="o">=</span> <span class="mi">2500</span> <span class="c1"># This is our output title vocabulary</span>
<span class="n">num_encoder_tokens</span> <span class="o">=</span> <span class="mi">5000</span> <span class="c1"># This is our input claim vocabulary</span>
<span class="n">encoder_seq_length</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1"># This is our limit for our input claim length</span>
<span class="n">decoder_seq_length</span> <span class="o">=</span> <span class="mi">22</span> <span class="c1"># This is our limit for our output title length - 20 + 2 for start/stop</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">PIK</span> <span class="o">=</span> <span class="s2">&quot;claim_and_title.data&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">PIK</span><span class="p">):</span>
    <span class="c1"># Download file</span>
    <span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">benhoyle</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="n">title_generation</span><span class="o">/</span><span class="n">claim_and_title</span><span class="o">.</span><span class="n">data</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">PIK</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loading data&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{0} samples loaded&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Adding start and stop tokens to output&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="p">,</span> <span class="s2">&quot;startseq {0} stopseq&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">An example title:&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;----&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;An example claim:&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Loading data
30000 samples loaded


Adding start and stop tokens to output


An example title: startseq System and method for session restoration at geo-redundant gateways stopseq
----
An example claim: 
1. A method for managing a backup service gateway (SGW) associated with a primary SGW, the method comprising:
periodically receiving from the primary SGW at least a portion of corresponding UE session state information, the received portion of session state information being sufficient to enable the backup SGW to indicate to an inquiring management entity that UEs having an active session supported by the primary SGW are in a live state; and
in response to a failure of the primary SGW, the backup SGW assuming management of IP addresses and paths associated with said primary SGW and transmitting a Downlink Data Notification (DDN) toward a Mobility Management Entity (MME) for each of said UEs having an active session supported by the failed primary SGW to detach from the network and reattach to the network, wherein each DDN causes the MME to send a detach request with a reattach request code to the respective UE.
</pre></div>


<p>Initially, we will begin with separate tokenizers for the input and the output. </p>
<p>(In later investigations it would seem sensible to use a common tokenizer for both input and output (as we can argue they both use the same pool of words). This could also help lead on to the use of a shared embedding later. But, we are getting ahead of ourselves...)</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.preprocessing</span> <span class="kn">import</span> <span class="n">text</span>
<span class="n">t_claim</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span>
                <span class="n">num_words</span><span class="o">=</span><span class="n">num_encoder_tokens</span><span class="p">,</span> 
                <span class="n">filters</span><span class="o">=</span><span class="s1">&#39;1.:;</span><span class="se">\n</span><span class="s1">()&#39;</span><span class="p">,</span>
                <span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">split</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span>
                <span class="n">char_level</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">oov_token</span><span class="o">=</span><span class="s2">&quot;&lt;UNK&gt;&quot;</span>
<span class="p">)</span>
<span class="n">X_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
<span class="n">t_claim</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">X_texts</span><span class="p">)</span>
<span class="n">X_seqs</span> <span class="o">=</span> <span class="n">t_claim</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_texts</span><span class="p">)</span>

<span class="n">t_title</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span> 
                <span class="n">num_words</span><span class="o">=</span><span class="n">num_decoder_tokens</span><span class="p">,</span>
                <span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">char_level</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">oov_token</span><span class="o">=</span><span class="s2">&quot;&lt;UNK&gt;&quot;</span>
<span class="p">)</span>
<span class="n">Y_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
<span class="n">t_title</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">Y_texts</span><span class="p">)</span>
<span class="n">Y_seqs</span> <span class="o">=</span> <span class="n">t_title</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">Y_texts</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our input sequences (claims) have a vocabulary of {0} words&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">max</span><span class="p">([</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_claim</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()])))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our output sequences (titles) have a vocabulary of {0} words&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">max</span><span class="p">([</span><span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">t_title</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()])))</span>
</pre></div>


<div class="highlight"><pre><span></span>Our input sequences (claims) have a vocabulary of 49376 words
Our output sequences (titles) have a vocabulary of 11080 words
</pre></div>


<div class="highlight"><pre><span></span><span class="n">filtered_seqs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_seqs</span><span class="p">,</span> <span class="n">Y_seqs</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">encoder_seq_length</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">decoder_seq_length</span><span class="p">]</span>
<span class="n">X_seqs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">filtered_seqs</span><span class="p">]</span>
<span class="n">Y_seqs</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">filtered_seqs</span><span class="p">]</span>

<span class="n">X_length</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_seqs</span><span class="p">]</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">X_length</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our longest input sequence is {0} tokens long.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>

<span class="n">Y_length</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y_seqs</span><span class="p">]</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Y_length</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our longest output sequence is {0} tokens long.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Our longest input sequence is 300 tokens long.
Our longest output sequence is 22 tokens long.
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">X_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_seqs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">Y_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Y_seqs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>1. A method for managing a backup service gateway (SGW) associated with a primary SGW, the method comprising:
periodically receiving from the primary SGW at least a portion of corresponding UE session state information, the received portion of session state information being sufficient to enable the backup SGW to indicate to an inquiring management entity that UEs having an active session supported by the primary SGW are in a live state; and
in response to a failure of the primary SGW, the backup SGW assuming management of IP addresses and paths associated with said primary SGW and transmitting a Downlink Data Notification (DDN) toward a Mobility Management Entity (MME) for each of said UEs having an active session supported by the failed primary SGW to detach from the network and reattach to the network, wherein each DDN causes the MME to send a detach request with a reattach request code to the respective UE.

 [2, 33, 9, 584, 2, 552, 95, 1217, 37, 20, 2, 363, 1, 33, 25, 1953, 56, 19, 1, 363, 14, 24, 2, 70, 3, 49, 3528, 364, 118, 244, 1, 86, 70, 3, 364, 118, 27, 46, 2315, 4, 754, 1, 552, 4, 944, 4, 11, 160, 275, 23, 58, 11, 500, 364, 1542, 15, 1, 363, 48, 6, 2, 1852, 118, 5, 6, 68, 4, 2, 974, 3, 1, 363, 1, 552, 160, 3, 621, 735, 5, 1051, 37, 20, 16, 363, 5, 272, 2, 10, 485, 1603, 2, 160, 275, 9, 28, 3, 16, 58, 11, 500, 364, 1542, 15, 1, 2422, 363, 4, 19, 1, 60, 5, 4, 1, 305, 17, 28, 900, 1, 4, 736, 2, 62, 20, 2, 62, 98, 4, 1, 100, 3528]
startseq System and method for session restoration at geo-redundant gateways stopseq [1, 6, 3, 5, 4, 383, 1183, 166, 775, 2]
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Pad the data</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_seqs</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">encoder_seq_length</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">Y_seqs</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">decoder_seq_length</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our X data has shape {0} and our Y data has shape {1}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Our</span> <span class="o">X</span> <span class="n">data</span> <span class="k">has</span> <span class="nb">shape</span> (<span class="mi">25529</span>, <span class="mi">300</span>) <span class="o">and</span> <span class="k">our</span> <span class="n">Y</span> <span class="n">data</span> <span class="k">has</span> <span class="nb">shape</span> (<span class="mi">25529</span>, <span class="mi">22</span>)
</pre></div>


<h2>Models</h2>
<p>We have a number of models we can try out. Initially we can run the models with minimal tweaking. Then later we can expand on the better models.</p>
<p>The models we can use are as follows:</p>
<ol>
<li>The Chollet/Brownlee Model based on the <a href="https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/">sequential encoder-decoder system</a> as described by Jason Brownlee of Machine Learning Mastery, which is in turn based on a <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">sequence-to-sequence model</a> as described by Francois Chollet; and</li>
<li>The Ludwig Model <a href="https://github.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras">chatbot encoder-decoder model</a> as described by Oswaldo Ludwig.</li>
</ol>
<p>All use Keras as this point. This helps to simplify our model and let us experiment with the basics.</p>
<p>The original model in 1. is designed based on character data. We can either use at the character level or adapt to use word-level features, e.g. as suggested at the bottom of the Chollet blog post.</p>
<p>The model in 2. uses word-level features. We are thus able to use this with less adaptation.</p>
<h3>Ludwig Model</h3>
<p>There are actually two models described by Oswaldo. A first introductory model and a second model that uses an additional adversarial network. The first model is easier to understand so we will start with that.</p>
<p>The first introductory model has the following features:</p>
<ul>
<li>A source text input model based around an Embedding layer and an LSTM. The claim features (i.e. words as integers) are projected to a dense vector by the embedding layer and then fed into the LSTM. The last output timestep of the LSTM is then taken as the output. The output of the LSTM can be thought of as a "context vector" that represents the claim.</li>
<li>A summary input model. This is similar to the source text input model. It has an Embedding layer and an LSTM. It takes a full or partial sequence of integers representing the words in our titles. The output of the last timestep is take as the output of the LSTM. The output of the LSTM can be thought of as an "answer vector" that represents a state of a current answer in time.</li>
<li>The output of both models, i.e. the context vector and the answer vector are concatenated and fed into a feed forward or Dense layer that outputs a vector the size of our title vocabulary. A softmax activation is used to generate a pseudo-probability value. The output of the model is thus a vector of probabilities across the vocabulary of the title. This can then be compared with a one-hot encoding of the actual word.</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># imports</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">concatenate</span>

<span class="n">y_vocab_len</span> <span class="o">=</span> <span class="n">num_decoder_tokens</span> <span class="c1"># This is our output title vocabulary</span>
<span class="n">X_vocab_len</span> <span class="o">=</span> <span class="n">num_encoder_tokens</span> <span class="c1"># This is our input claim vocabulary</span>
<span class="n">X_max_len</span> <span class="o">=</span> <span class="n">encoder_seq_length</span> <span class="c1"># This is our limit for our input claim length</span>
<span class="n">y_max_len</span> <span class="o">=</span> <span class="n">decoder_seq_length</span> <span class="c1"># This is our limit for our output title length - 20 + 2 for start/stop</span>

<span class="c1"># source text input model</span>
<span class="n">inputs1</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_max_len</span><span class="p">,))</span>
<span class="n">am1</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">X_vocab_len</span><span class="p">,</span> <span class="mi">128</span><span class="p">)(</span><span class="n">inputs1</span><span class="p">)</span>
<span class="n">am2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">)(</span><span class="n">am1</span><span class="p">)</span>
<span class="c1"># summary input model</span>
<span class="n">inputs2</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">y_max_len</span><span class="p">,))</span>
<span class="n">sm1</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">y_vocab_len</span><span class="p">,</span> <span class="mi">128</span><span class="p">)(</span><span class="n">inputs2</span><span class="p">)</span>
<span class="n">sm2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">)(</span><span class="n">sm1</span><span class="p">)</span>
<span class="c1"># decoder output model</span>
<span class="n">decoder1</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">am2</span><span class="p">,</span> <span class="n">sm2</span><span class="p">])</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">y_vocab_len</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">decoder1</span><span class="p">)</span>
<span class="c1"># tie it together [article, summary] [word]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs1</span><span class="p">,</span> <span class="n">inputs2</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 300)          0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 22)           0                                            
__________________________________________________________________________________________________
embedding_1 (Embedding)         (None, 300, 128)     640000      input_1[0][0]                    
__________________________________________________________________________________________________
embedding_2 (Embedding)         (None, 22, 128)      320000      input_2[0][0]                    
__________________________________________________________________________________________________
lstm_1 (LSTM)                   (None, 128)          131584      embedding_1[0][0]                
__________________________________________________________________________________________________
lstm_2 (LSTM)                   (None, 128)          131584      embedding_2[0][0]                
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 256)          0           lstm_1[0][0]                     
                                                                 lstm_2[0][0]                     
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2500)         642500      concatenate_1[0][0]              
==================================================================================================
Total params: 1,865,668
Trainable params: 1,865,668
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>


<p>The model is fairly simple. However, a little bit of work is required to prepare our data for training. There are two main issues we need to address:</p>
<ul>
<li>The model requires a one-hot encoding for each word in the title. If our vocabulary is 2500 (relatively small) and our titles are limited to 20 words, we have a matrix of 50,000 items per data sample. If we have 25,000 data samples then we have 1.25 billion items. If each item is a 32 or 64-bit float we have a memory issue.</li>
<li>The summary model is designed to take partially-complete sequences as the output sequence is built. We thus need to train on various levels of completeness. For example, if our title is 20 words long, we train with 1 word to predict the second word, 2 words to predict the third word, 3 words to predict the fourth word etc.</li>
</ul>
<p>To address the first issue, we train in smaller sets. For example, we can group 100 examples together and train on these.</p>
<p>To address the second issue, we build up sets of partial titles for each training example.</p>
<div class="highlight"><pre><span></span><span class="c1"># We need to split into train and test data</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># seed for reproducing same results</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># split the data into training (80%) and testing (20%)</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_set</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Generate the data for training/validation from X and Y.</span>
<span class="sd">    i_end is the end of the set, i is the start.&quot;&quot;&quot;</span>
    <span class="n">set_size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">limit_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i_end</span><span class="p">]:</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">sent</span><span class="o">==</span><span class="mi">2</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1">#  the position od the symbol EOS</span>
        <span class="n">set_size</span> <span class="o">+=</span> <span class="n">limit</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">limit_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limit</span><span class="p">)</span>

    <span class="c1"># We need to change this bit to set our array size based on the limit values</span>
    <span class="c1"># Generate blank arrays for the set</span>
    <span class="n">I_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">set_size</span><span class="p">,</span> <span class="n">X_max_len</span><span class="p">))</span>
    <span class="n">I_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">set_size</span><span class="p">,</span> <span class="n">y_max_len</span><span class="p">))</span>
    <span class="c1"># This below is a big array</span>
    <span class="n">Y_set</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">set_size</span><span class="p">,</span> <span class="n">y_vocab_len</span><span class="p">))</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Now we want to create, for each sample, a set of examples for each word in the title</span>
    <span class="c1"># Have we just been training on 0 to 100?!?!</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">i_end</span> <span class="o">-</span> <span class="n">i</span><span class="p">)):</span>
        <span class="c1"># for each X and y in set of NB_SET </span>

        <span class="c1"># We need to build the input for the second encoder for the next word in y</span>
        <span class="c1"># I.e. for word 3 in the title the input2 consists of words 1 and 2 (using teacher forcing)</span>

        <span class="c1"># Get length of current title - i.e. where the integer = 2 = stopseq</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">limit_list</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>

        <span class="c1"># We only need to create examples up to the length of the title </span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">limit</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>

            <span class="c1"># Generate our one-hot y out</span>
            <span class="n">one_hot_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_vocab_len</span><span class="p">))</span>
            <span class="c1"># This builds our one-hot generation into our training loop</span>
            <span class="c1"># The l and m respectively iterate through the samples and the output sequence elements</span>
            <span class="n">one_hot_out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">i</span><span class="p">][</span><span class="n">m</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># Create a blank row/array for a partial input for our summary model - this is fed into the decoder</span>
            <span class="c1"># It is of the same size as our title</span>
            <span class="n">partial_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_max_len</span><span class="p">))</span>
            <span class="c1"># Because we are zero padding add words up to m to end - DOES THIS STILL WORK IF WE ZERO PAD</span>
            <span class="c1"># AT THE END? - Yes but we just feed the words with zeros first?</span>
            <span class="n">partial_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">m</span><span class="p">:]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">]</span>

            <span class="c1"># This fills in each sample of the training data, i.e. count increments up to set size</span>
            <span class="n">I_1</span><span class="p">[</span><span class="n">count</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">i</span><span class="p">]</span>
            <span class="n">I_2</span><span class="p">[</span><span class="n">count</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">partial_input</span>
            <span class="n">Y_set</span><span class="p">[</span><span class="n">count</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">one_hot_out</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Shuffle the I_1, I_2 and Y_set vectors for better training - trick from RL</span>
        <span class="c1"># - see here - np.take(X,np.random.permutation(X.shape[0]),axis=0,out=X);</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">I_1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">I_1</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">I_1</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">I_2</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">I_2</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">Y_set</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">Y_set</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">I_1</span><span class="p">,</span> <span class="n">I_2</span><span class="p">,</span> <span class="n">Y_set</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Basing training in sets code on here - https://github.com/ChunML/seq2seq/blob/master/seq2seq.py</span>

<span class="c1"># Function to look for saved weights file</span>
<span class="k">def</span> <span class="nf">find_checkpoint_file</span><span class="p">(</span><span class="n">folder</span><span class="p">):</span>
    <span class="n">checkpoint_file</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;kerascheckpoint&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">checkpoint_file</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="n">modified_time</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getmtime</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">checkpoint_file</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">checkpoint_file</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">modified_time</span><span class="p">)]</span>

<span class="c1"># Finding trained weights of previous epoch if any</span>
<span class="n">saved_weights</span> <span class="o">=</span> <span class="n">find_checkpoint_file</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="n">k_start</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># If any trained weight was found, then load them into the model</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[INFO] Saved weights found, loading...&#39;</span><span class="p">)</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="n">saved_weights</span><span class="p">[</span><span class="n">saved_weights</span><span class="o">.</span><span class="n">rfind</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">saved_weights</span><span class="o">.</span><span class="n">rfind</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span>
    <span class="n">k_start</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># So instead of X we have [inputs1, inputs2] - this is where we need to fold in </span>
<span class="c1"># - https://github.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras/blob/master/train_bot.py</span>

<span class="c1"># So we have inputs2 that build up - we have a set of inputs2 up to the length of inputs2</span>

<span class="c1"># We need to refactor some of the loops below as functions - we can then apply to test data to generate a validation set</span>
</pre></div>


<div class="highlight"><pre><span></span>[INFO] Saved weights found, loading...
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span> <span class="c1"># Depends on GPU - most values are around this 32-128 </span>
<span class="n">NB_EPOCH</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># Number of examples to group together in a set - 100 is fast / 1000 is too much on an 8-core i7 laptop</span>
<span class="c1"># I think 100 is good - 250 takes a time to generate the sets of test data</span>
<span class="n">NB_SET</span> <span class="o">=</span> <span class="mi">250</span>

<span class="n">i_end</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">num_test</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># Initialise history of accuracy</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="c1"># Continue from loaded epoch number or new epoch if not loaded</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_start</span><span class="p">,</span> <span class="n">NB_EPOCH</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Shuffling the training data every epoch to avoid local minima</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y_train</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="c1"># This for loop rotates through NB_SET samples at a time to avoid memory issues</span>
    <span class="c1"># E.g. Training 100 sequences at a time</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">,</span> <span class="n">NB_SET</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">NB_SET</span> <span class="o">&gt;=</span> <span class="n">num_examples</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">num_examples</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">NB_SET</span>

        <span class="c1"># Generate a range for the test data</span>
        <span class="n">i_test</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>
        <span class="n">i_test_end</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i_end</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>

        <span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">,</span> <span class="n">Y_set_train</span> <span class="o">=</span> <span class="n">generate_set</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">,</span> <span class="n">Y_set_test</span> <span class="o">=</span> <span class="n">generate_set</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">i_test_end</span><span class="p">,</span> <span class="n">i_test</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[INFO] Training model: epoch {} - {}/{} samples&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">))</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="p">[</span><span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">],</span> 
            <span class="n">Y_set_train</span><span class="p">,</span> 
            <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">],</span> <span class="n">Y_set_test</span><span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
            <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
        <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
        <span class="c1"># Get history and apppend new data to running set here</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s1">&#39;kerascheckpoint_epoch_{}.hdf5&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>[INFO] Training model: epoch 2 - 0/20423 samples
Train on 2463 samples, validate on 658 samples
Epoch 1/1
2463/2463 [==============================] - 23s 9ms/step - loss: 2.5461 - val_loss: 3.0267
[INFO] Training model: epoch 2 - 250/20423 samples
Train on 2552 samples, validate on 607 samples
Epoch 1/1
2552/2552 [==============================] - 24s 9ms/step - loss: 2.5839 - val_loss: 2.8048
[INFO] Training model: epoch 2 - 500/20423 samples
Train on 2446 samples, validate on 591 samples
Epoch 1/1
2446/2446 [==============================] - 24s 10ms/step - loss: 2.4230 - val_loss: 2.7162

...YADAYADAYADA...

[INFO] Training model: epoch 2 - 20000/20423 samples
Train on 2422 samples, validate on 631 samples
Epoch 1/1
2422/2422 [==============================] - 25s 10ms/step - loss: 2.4810 - val_loss: 2.8054
[INFO] Training model: epoch 2 - 20250/20423 samples
Train on 1810 samples, validate on 472 samples
Epoch 1/1
1810/1810 [==============================] - 19s 10ms/step - loss: 2.4741 - val_loss: 3.0421
</pre></div>


<p>At the end of a set of training:</p>
<div class="highlight"><pre><span></span>[INFO] Training model: epoch 2 - 19750/20423 samples
Train on 2514 samples, validate on 606 samples
Epoch 1/1
2514/2514 [==============================] - 26s 10ms/step - loss: 2.4726 - val_loss: 2.8416
[INFO] Training model: epoch 2 - 20000/20423 samples
Train on 2422 samples, validate on 631 samples
Epoch 1/1
2422/2422 [==============================] - 25s 10ms/step - loss: 2.4810 - val_loss: 2.8054

[INFO] Training model: epoch 2 - 20250/20423 samples
Train on 1810 samples, validate on 472 samples
Epoch 1/1
1810/1810 [==============================] - 19s 10ms/step - loss: 2.4741 - val_loss: 3.0421
</pre></div>


<p>Now we've trained our model we need some code to test it.</p>
<div class="highlight"><pre><span></span><span class="c1"># Set up dictionary to translate indices to words</span>
<span class="n">y_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">char</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">t_title</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">)</span>

<span class="n">x_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">char</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">t_claim</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">seq2text</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">text</span> <span class="o">+</span> <span class="n">w</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span>
    <span class="k">return</span> <span class="n">text</span>

<span class="k">def</span> <span class="nf">greedy_decoder</span><span class="p">(</span><span class="n">X_seq</span><span class="p">):</span>
    <span class="c1"># reformat input seq</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_max_len</span><span class="p">))</span>
    <span class="n">input_seq</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">X_seq</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ans_partial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_max_len</span><span class="p">))</span>
    <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1">#  the index of the symbol BOS (begin of sentence)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_max_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ye</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">ans_partial</span><span class="p">])</span>
        <span class="n">yel</span> <span class="o">=</span> <span class="n">ye</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">yel</span><span class="p">)</span>
        <span class="n">mp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ye</span><span class="p">)</span>
        <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">mp</span>
        <span class="k">if</span> <span class="n">mp</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>  <span class="c1">#  the index of the symbol EOS (end of sentence)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">flag</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>    
            <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">p</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>

<span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">text</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">greedy_decoder</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {} (with prob {}). </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: live dual validated independently verified dynamic match audit system comprising first device for transmitting an element effort event data, which data details an effort of an element on an event, and

Predicted title is: system and method for generating a plurality of  (with prob 2.988668487895419e-05). 
 Test title is: match system and event data recorder method thereof  
---

Sample of claim text: method comprising facilitating receipt of two or more images of scene, two or more images being associated with different capture parameters determining intensity at corresponding pixel locations of a

Predicted title is: image processing apparatus and image processing method apparatus and computer readable medium  (with prob 0.007207380954881892). 
 Test title is: method apparatus and computer program product for generating images of scenes having high dynamic range  
---

Sample of claim text: an information terminal for reducing touch point reading errors and releasing security lock of information terminal, comprising touch panel that displays plurality of touch points coordinate determini

Predicted title is: touch panel and method of controlling the same  (with prob 0.0002631053677347134). 
 Test title is: information terminal  
---

Sample of claim text: method comprising receiving message having coded therein information identifying desired transport mechanism for request and/or response, received message including portion and portion processing, by

Predicted title is: method and system for providing a network  (with prob 3.861037278558291e-05). 
 Test title is: request and response via in a service oriented pipeline architecture for a request response message exchange pattern  
---

Sample of claim text: an apparatus, comprising block processing pipeline implemented in video encoder circuit and configured to process blocks of pixels from video frames wherein block processing pipeline comprises block i

Predicted title is: method and apparatus for generating a three dimensional image  (with prob 7.063415872216406e-07). 
 Test title is: encoding blocks in video frames containing text using of  
---
</pre></div>


<h4>Comments on Results</h4>
<p>The predicted titles seem to be well-formatted and make sense. The problem is they seem to be unrelated to the actual title!</p>
<p>What I think is going on is the second LSTM is over-fitting on the training data. Hence, what you have is a system that memorises the training titles, then picks a memorised training title based on the output of the first LSTM. </p>
<p>It also appears that the link between the first and second LSTMs is weak - the first encoding or "context vector" does not appear to be heavily influencing the title generation. The prediction of the dense layer seems to be mainly based on the second LSTM. Hence, the dense layer and second LSTM in tandem are over-fitting.</p>
<p>This is backed up by the fact that when we do not shuffle the training data we have rapid convergence to a low loss. This is indicative of over-fitting. It is predicted that by not shuffling we should get rapid convergence but a maintained high validation loss.</p>
<h4>Options for Further Investigation</h4>
<p>We can try adding in shared word embeddings based on Glove encodings (as per the second Ludwig model). The post <a href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/">here</a> shows how we convert the text file of Glove encodings into a matrix given a vocabulary from our data.</p>
<p>We can try adding the adversarial discriminator.</p>
<hr>
<h3>Chollet/Brownlee Model</h3>
<p>This is more of a true sequence-to-sequence model, and is thus slightly more involved.</p>
<p>Our model consists of two portions - a portion for training and a portion for inference (i.e. for actually predicting new titles).</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">argmax</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array_equal</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="k">def</span> <span class="nf">target_one_hot</span><span class="p">(</span><span class="n">input_seqs</span><span class="p">,</span> <span class="n">seq_max_len</span><span class="p">,</span> <span class="n">vocab_len</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Convert a sequence of integers to a one element shifted sequence of one-hot vectors.&quot;&quot;&quot;</span>
    <span class="n">one_hot_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">input_seqs</span><span class="p">),</span> <span class="n">seq_max_len</span><span class="p">,</span> <span class="n">vocab_len</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_seqs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">word_int</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sequence</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Shift decoder target get so it is one ahead</span>
                <span class="n">one_hot_out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">word_int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">one_hot_out</span>

<span class="c1"># We need to convert this for our present problem - this is similar to our generate dataset above</span>
<span class="c1"># prepare data for the LSTM</span>
<span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return encoder_input_data, decoder_input_data, and decoder_target_data, latter as one-hot&quot;&quot;&quot;</span>
    <span class="n">encoder_input_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i_end</span><span class="p">]</span>
    <span class="n">decoder_input_data</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i_end</span><span class="p">]</span>
    <span class="n">decoder_target_data</span> <span class="o">=</span> <span class="n">target_one_hot</span><span class="p">(</span><span class="n">decoder_input_data</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoder_input_data</span><span class="p">,</span> <span class="n">decoder_input_data</span><span class="p">,</span> <span class="n">decoder_target_data</span>

<span class="c1"># returns train, inference_encoder and inference_decoder models</span>
<span class="k">def</span> <span class="nf">define_models</span><span class="p">(</span><span class="n">num_encoder_tokens</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">):</span>
    <span class="c1"># define training encoder</span>
    <span class="c1"># Define an input sequence and process it.</span>
    <span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
    <span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">num_encoder_tokens</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)(</span><span class="n">encoder_inputs</span><span class="p">)</span>
    <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)(</span><span class="n">encoder_embedding</span><span class="p">)</span>
    <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

    <span class="c1"># Set up the decoder, using `encoder_states` as initial state.</span>
    <span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
    <span class="c1"># Possibly share the embedding below</span>
    <span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)(</span><span class="n">decoder_inputs</span><span class="p">)</span>
    <span class="n">decoder_lstm</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_lstm</span><span class="p">(</span><span class="n">decoder_embedding</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">)</span>
    <span class="n">decoder_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
    <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">)</span>

    <span class="c1"># Define the model that will turn</span>
    <span class="c1"># `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span> <span class="n">decoder_outputs</span><span class="p">)</span>

    <span class="c1"># define inference encoder</span>
    <span class="n">encoder_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">)</span>
    <span class="c1"># define inference decoder</span>
    <span class="n">decoder_state_input_h</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
    <span class="n">decoder_state_input_c</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
    <span class="n">decoder_states_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">decoder_state_input_h</span><span class="p">,</span> <span class="n">decoder_state_input_c</span><span class="p">]</span>
    <span class="c1"># Need to adjust this line for the embedding</span>
    <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">decoder_lstm</span><span class="p">(</span><span class="n">decoder_embedding</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">decoder_states_inputs</span><span class="p">)</span>
    <span class="n">decoder_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>
    <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">)</span>
    <span class="n">decoder_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">decoder_inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">decoder_states_inputs</span><span class="p">,</span> <span class="p">[</span><span class="n">decoder_outputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">decoder_states</span><span class="p">)</span>
    <span class="c1"># return all models</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">encoder_model</span><span class="p">,</span> <span class="n">decoder_model</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">set_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Code to train model in sets of set_size.&quot;&quot;&quot;</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">num_test</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Loop here to avoid memory issues with the target one hot vector</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">,</span> <span class="n">set_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">set_size</span> <span class="o">&gt;=</span> <span class="n">num_examples</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">num_examples</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">set_size</span>
        <span class="c1"># Generate a range for the test data</span>
        <span class="n">i_test</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>
        <span class="n">i_test_end</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i_end</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>
        <span class="c1"># Generate small sets of train and test data</span>
        <span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">,</span> <span class="n">Y_set_train</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
        <span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">,</span> <span class="n">Y_set_test</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">i_test</span><span class="p">,</span> <span class="n">i_test_end</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[INFO] Training model: {}/{} samples&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">))</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="p">[</span><span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">],</span> 
            <span class="n">Y_set_train</span><span class="p">,</span> 
            <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">],</span> <span class="n">Y_set_test</span><span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> 
            <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
        <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span>

<span class="c1"># define model</span>
<span class="n">train</span><span class="p">,</span> <span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span> <span class="o">=</span> <span class="n">define_models</span><span class="p">(</span><span class="n">num_encoder_tokens</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">train</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># setup variables</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">set_size</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--------</span><span class="se">\n</span><span class="s2"> Epoch - &quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
    <span class="n">train</span><span class="p">,</span> <span class="n">tl</span><span class="p">,</span> <span class="n">vl</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">set_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
    <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">tl</span>
    <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">vl</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s2">&quot;chollet_weights.hdf5&quot;</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>--------
 Epoch -  0
[INFO] Training model: 0/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 82s 16ms/step - loss: 1.3725 - acc: 0.7448 - val_loss: 1.7861 - val_acc: 0.7027
[INFO] Training model: 5000/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 84s 17ms/step - loss: 1.3737 - acc: 0.7435 - val_loss: 1.7589 - val_acc: 0.7081
[INFO] Training model: 10000/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 87s 17ms/step - loss: 1.3650 - acc: 0.7461 - val_loss: 1.7568 - val_acc: 0.7059
[INFO] Training model: 15000/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 87s 17ms/step - loss: 1.3451 - acc: 0.7480 - val_loss: 1.7507 - val_acc: 0.7123
[INFO] Training model: 20000/20423 samples
Train on 423 samples, validate on 106 samples
Epoch 1/1
423/423 [==============================] - 7s 16ms/step - loss: 1.3241 - acc: 0.7517 - val_loss: 1.8438 - val_acc: 0.7028

--------

...YADAYADAYADA....

--------
 Epoch -  9
[INFO] Training model: 0/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 90s 18ms/step - loss: 1.2092 - acc: 0.7635 - val_loss: 1.8768 - val_acc: 0.6985
[INFO] Training model: 5000/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 88s 18ms/step - loss: 1.2145 - acc: 0.7622 - val_loss: 1.8442 - val_acc: 0.7035
[INFO] Training model: 10000/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 89s 18ms/step - loss: 1.2038 - acc: 0.7641 - val_loss: 1.8421 - val_acc: 0.7036
[INFO] Training model: 15000/20423 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 89s 18ms/step - loss: 1.1822 - acc: 0.7671 - val_loss: 1.8415 - val_acc: 0.7073
[INFO] Training model: 20000/20423 samples
Train on 423 samples, validate on 106 samples
Epoch 1/1
423/423 [==============================] - 8s 19ms/step - loss: 1.1642 - acc: 0.7720 - val_loss: 1.9424 - val_acc: 0.6990
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># summarize history for accuracy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="https://benhoyle.github.io/images/TG_3_output_31_0.png"></p>
<p>So here we want to stop after around 50 iterations - this is where our training and test loss diverge and marks where over-fitting begins.</p>
<p>We then need the inference code to allow prediction and decoding.</p>
<p>The general pattern is as follows:</p>
<ul>
<li>Predict the state with infenc using an input X vector.</li>
<li>Create an initial target vector - is this 0s with first entry the startseq token 1? In the one-hot case, it's a one-hot encoding of one character - would our equivalent just be <code>array([1])</code></li>
<li>while stop condition is false, where stop condition = len=max_y_len or token=2=stopseq<ul>
<li>use infdec to predict yhat and return the state of the decoder h and c </li>
<li>remember the yhat </li>
<li>set yhat as the next target sequence and pass generated state</li>
</ul>
</li>
</ul>
<p>I.e. I think we can similar code to Brownlee's or Chollet's model.</p>
<div class="highlight"><pre><span></span><span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">,</span> <span class="n">Y_set_train</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
<span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">,</span> <span class="n">Y_set_test</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">I_1_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">I_2_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_set_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">I_1_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">I_2_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y_set_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(0, 300) (0, 22) (0, 22, 2500)
(0, 300) (0, 22) (0, 22, 2500)
</pre></div>


<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">decoder_seq_length</span><span class="p">):</span>
    <span class="c1"># encode</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">infenc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="c1"># start of sequence input</span>
    <span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># collect predictions</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">decoder_seq_length</span><span class="p">):</span>
        <span class="c1"># predict next char</span>
        <span class="n">yhat</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">infdec</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">target_seq</span><span class="p">]</span> <span class="o">+</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># update state</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>
        <span class="c1"># update target sequence - this needs to be the argmax</span>
        <span class="n">next_int</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_int</span><span class="p">)</span>
        <span class="c1"># It seems like we throw a lot of information away here - can we build in the probabilities?</span>
        <span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="n">next_int</span><span class="p">])</span>
        <span class="c1"># Check for stopping character</span>
        <span class="k">if</span> <span class="n">next_int</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">pred_seq</span> <span class="o">=</span> <span class="n">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">decoder_seq_length</span><span class="p">)</span>
    <span class="n">predicted_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {}. </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_text</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: method for authenticating an identity of cardholder in financial transaction initiated by cardholder with merchant via first communication medium, said method comprising receiving purchase authenticat

Predicted title is: a more more dimensional more more more more more more more dimensional dimensional more more more more more dimensional dimensional more more . 
 Test title is: methods and systems for authenticating an identity of a in a financial transaction  
---

Sample of claim text: an array substrate including display area and peripheral area, wherein display area includes plurality of thin film transistors provided on base substrate, first transparent electrodes electrically co

Predicted title is: a more more dimensional more more more more more more more dimensional dimensional more more more more more dimensional dimensional more more . 
 Test title is: array substrate and manufacturing method thereof touch display device  
---

Sample of claim text: method for identifying conditional actions in business process comprising determining plurality of pairs of text fragments that respectively include text fragments that are similar according to pre-de

Predicted title is: a more more dimensional more more more more more more more dimensional dimensional more more more more more dimensional dimensional more more . 
 Test title is: identifying and conditional actions in business processes  
---

Sample of claim text: method of processing merchandise return comprising receiving request to merchandise return made by consumer at point of return determining with one or more computer processors whether to return made b

Predicted title is: a more more dimensional more more more more more more more dimensional dimensional more more more more more dimensional dimensional more more . 
 Test title is: systems and methods for data collection at a point of return  
---

Sample of claim text: method, comprising detecting, on touch screen of an information device, user input determining, using processor, that user input occurs within predetermined edge region of touch screen adjusting, usin

Predicted title is: a more more dimensional more more more more more more more dimensional dimensional more more more more more dimensional dimensional more more . 
 Test title is: detecting and filtering edge touch inputs  
---
</pre></div>


<p>We have a lower loss than the Ludwig model but our output is the same for all predicted titles: a seemingly random set of repetitive phrases.</p>
<p>I have seen this form of behaviour before. One method of avoiding it was to play with the temperature of the sampling. This is explained <a href="https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling">here</a> as set out below.</p>
<p><img alt="temperature.png" src="https://benhoyle.github.io/images/temperature.png"></p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># helper function to sample an index from a probability array</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="n">exp_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">exp_preds</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_preds</span><span class="p">)</span>
    <span class="n">probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">state</span> <span class="o">=</span> <span class="n">infenc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># start of sequence input</span>
<span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># collect predictions</span>
<span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">decoder_seq_length</span><span class="p">):</span>
    <span class="c1"># predict next char</span>
    <span class="n">yhat</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">infdec</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">target_seq</span><span class="p">]</span> <span class="o">+</span> <span class="n">state</span><span class="p">)</span>
    <span class="c1"># update state</span>
    <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>
    <span class="c1"># update target sequence - this needs to be the argmax</span>
    <span class="n">next_int</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
    <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_int</span><span class="p">)</span>
    <span class="c1"># It seems like we throw a lot of information away here - can we build in the probabilities?</span>
    <span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="n">next_int</span><span class="p">])</span>
    <span class="c1"># Check for stopping character</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>[1226, 1415, 2180, 1370, 873, 1059, 18, 414, 364, 1926, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</pre></div>


<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">decoder_seq_length</span><span class="p">,</span> <span class="n">temp</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># encode</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">infenc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="c1"># start of sequence input</span>
    <span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># collect predictions</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">decoder_seq_length</span><span class="p">):</span>
        <span class="c1"># predict next char</span>
        <span class="n">yhat</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">infdec</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">target_seq</span><span class="p">]</span> <span class="o">+</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># update state</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>
        <span class="c1"># update target sequence - this needs to be the argmax</span>
        <span class="n">next_int</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">temp</span><span class="p">)</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_int</span><span class="p">)</span>
        <span class="c1"># It seems like we throw a lot of information away here - can we build in the probabilities?</span>
        <span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="n">next_int</span><span class="p">])</span>
        <span class="c1"># Check for stopping character</span>
        <span class="k">if</span> <span class="n">next_int</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">pred_seq</span> <span class="o">=</span> <span class="n">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">decoder_seq_length</span><span class="p">)</span>
    <span class="n">predicted_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {}. </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_text</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: character string recognition method for recognizing character string comprising steps of imaging character string on medium to obtain image data first step in which first projection data of image data

Predicted title is: fidelity awareness screening using regions filtering said program . 
 Test title is: character string recognition method and device  
---

Sample of claim text: device for selectively modifying transmission performance of frame data having data size and preset transmission clock rate, frame data being transmitted through transmission interface toward an image

Predicted title is: dependent accessory candidate lock assignment algorithms initiated devices using manufacture . 
 Test title is: modification device and method for selectively modifying transmission performance of image frame data  
---

Sample of claim text: an apparatus for use as switch, comprising ring tag including ring element with an rfid circuit, ring tag attached to rotatable on rotation axis to place ring tag into first position, wherein ring tag

Predicted title is: refresh similarity coverage serving work hosted secure end diagnosis malicious cryptographic attribute claims tissue controller therefor . 
 Test title is: switch using radio frequency identification  
---

Sample of claim text: computer-implemented method, comprising receiving sensor data from user device, sensor data comprising image data depicting at least portion of user of user device, image data provided by an image cap

Predicted title is: tuning modified pertaining searching detecting scanner response . 
 Test title is: employing device sensor data to determine user characteristics  
---

Sample of claim text: method of prioritizing feed items based on rules, method comprising receiving input data, wherein input data comprises data or feed metrics receiving user input to provide one or more rules to feed it

Predicted title is: timeline abstraction matrix tests combining marks . 
 Test title is: rule based prioritization of social data  
---
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">pred_seq</span> <span class="o">=</span> <span class="n">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">decoder_seq_length</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">predicted_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {}. </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_text</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: method for providing service address space, comprising providing service with service address space attached to main processor, wherein main processor is provided with main address space, wherein serv

Predicted title is: networked engine memory secured same . 
 Test title is: providing service address space for diagnostics collection  
---

Sample of claim text: an image processing device comprising filter control unit configured to on basis of an prediction mode, filter processing turned on or off as to neighboring pixels that are located to current block fo

Predicted title is: determination virtual rfid swap manufacturing thin session mapping advanced wireless asset sequences telematics . 
 Test title is: image processing device and method  
---

Sample of claim text: processor implemented method for operating client computing device to allow user to interact with network sites over network interface, method comprising initiating via processor operation of browser

Predicted title is: analysis natural alternate adaptive personal methods of applications . 
 Test title is: system method apparatus and means for evaluating historical network activity  
---

Sample of claim text: method of converting enterprise resource planning data in database managed by an application and accessed through an application programming interface api and message agent api to data in java object

Predicted title is: multiple preparing peer estimating insertion signatures measurement . 
 Test title is: accessing a application over the internet using declarative language files  
---

Sample of claim text: method for information units, method comprises receiving, by storage system, write requests for writing information units to storage system wherein write requests are initiated by multiple accessing e

Predicted title is: fuel clustering dedicated length repair names view copy in web store engine . 
 Test title is: system method and a non transitory computer readable medium for a pre operation  
---
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">pred_seq</span> <span class="o">=</span> <span class="n">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">decoder_seq_length</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
    <span class="n">predicted_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {}. </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_text</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: method for using persistent party for comprising via computing device, global array according to hierarchical server architecture, wherein hierarchical server architecture enables user to create and h

Predicted title is: a volatile dimensional dimensional more more more more more more more dimensional determine dimensional more more more dimensional dimensional more more more . 
 Test title is: persistent party  
---

Sample of claim text: an input device comprising selecting section for selecting pair from among plurality of x electrodes and y electrodes disposed as being spaced apart from each other detecting section for detecting tou

Predicted title is: a volatile dimensional more more more dimensional more more more more more dimensional dimensional dimensional more more more more dimensional dimensional more . 
 Test title is: input device input control method program and electronic apparatus  
---

Sample of claim text: an image processing apparatus comprising designating unit adapted to as an image processing function to be applied to an image of interest, any of plurality of image processing functions including cop

Predicted title is: a more more volatile dimensional more more more more more more dimensional volatile dimensional dimensional dimensional image adjust dimensional transitory image volatile . 
 Test title is: image processing apparatus method and program  
---

Sample of claim text: an image processing apparatus comprising separation unit configured to separate image data into luminance signal and color difference signal decision unit configured to reference pixel, which is refer

Predicted title is: be . 
 Test title is: image processing apparatus  
---

Sample of claim text: method comprising identifying, by one or more computers, an image in first storage system determining, by one or more computers, whether second storage system includes an entry for image identifying r

Predicted title is: a dimensional volatile dimensional more more more more more dimensional more more dimensional more more more more more dimensional more more dimensional . 
 Test title is: methods and apparatus for automated object based image analysis and retrieval  
---
</pre></div>


<p>So low temperature reproduces the default results.</p>
<div class="highlight"><pre><span></span><span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">pred_seq</span> <span class="o">=</span> <span class="n">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">decoder_seq_length</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
    <span class="n">predicted_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {}. </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_text</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: computer-implemented method for analyzing data representative of media material having layout, comprising identifying block segments associated with body text in media material and determining which o

Predicted title is: electrode localization funds link being . 
 Test title is: methods and systems for analyzing data in media material having layout  
---

Sample of claim text: computer interface system comprising at least one camera operable to create an image of by user, comprising plurality of features and b processing module operable to receive created image of and deter

Predicted title is: unstructured log entity plural adapted performed . 
 Test title is: for camera calibration and as a gesture based 3d interface device  
---

Sample of claim text: computer-implemented method, comprising receiving, by computing system, multiple portions of text that were input into different types of fields associated with resource selecting i first threshold va

Predicted title is: preferred map merging horizontal pad refinement normal operands editing multicore health party storage communication terminal or management server memory . 
 Test title is: selectively processing user input  
---

Sample of claim text: mobile terminal comprising display memory configured to store at least one image fragment first camera user input unit configured to receive input and controller configured to cause display to display

Predicted title is: determining technical tomography initiating sequences behavior spatial particular electro managing pipeline assistance shared listing commerce user requesting arithmetic criteria domains insertion schema . 
 Test title is: mobile terminal and controlling method thereof  
---

Sample of claim text: an information handling system comprising plurality of ports for sending data to and receiving data from one or more devices one or more processors that are communicatively coupled to plurality of por

Predicted title is: mass diagram level user&#39;s development metal identification operable requests computer machine attached product . 
 Test title is: reducing internal fabric in switch fabric  
---
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">pred_seq</span> <span class="o">=</span> <span class="n">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">decoder_seq_length</span><span class="p">)</span>
    <span class="n">predicted_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {} </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_text</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: an id document comprising document core layer having two opposed surfaces and on at least one surface of document core layer, comprising top layer middle layer and bottom layer, wherein point of botto

Predicted title is: removal human partitioning related recording platforms  
 Test title is: id documents having a multi layered structure  
---

Sample of claim text: method for presenting story content relating to and location of interest, method comprising receiving query comprising an intersection criteria, intersection criteria comprising location and of intere

Predicted title is: item many step chips overlay transmitting  
 Test title is: systems and methods for collaborative in a virtual space  
---

Sample of claim text: method of forming an image using plurality of comprising receiving at least one raster of image pixels each having discrete value for each of for raster associated with defective modifying values of p

Predicted title is: held appearance execution acceleration site containing reporting common remote it nand configurations  
 Test title is: replacing a  
---

Sample of claim text: an integrated circuit ic comprising digital circuit comprising derived clock circuit configured to receive root clock having frequency and single phase, wherein d is divide factor for root wherein der

Predicted title is: detect compliance separation  
 Test title is: apparatus and method for reducing interference signals in an integrated circuit using  
---

Sample of claim text: method for using secure socket layer session from pool of sessions shared between method comprising receiving, by first intermediary device, information on one or more sessions of pool established by

Predicted title is: different appearance interference characters via modules including web sequences  
 Test title is: systems and methods of using pools for acceleration  
---
</pre></div>


<h4>Comments on Results</h4>
<p>Interestingly these results vary a little from the Ludwig model.</p>
<p>We generally have a lower loss (~1.3 on training data, compared to ~2.5 for Ludwig model). The results from the present model seem less a memorisation of previous titles - they include more grammatical variations and seem to be more mixtures of different title portions.</p>
<p>However, the answers seem less well-formed than the Ludwig model, making them subjectively appear worse.</p>
<p>There again appears a disconnect between the claim and the predicted title. </p>
<p>The model appears to start overfitting around 50 iterations (~4 to 5 epochs based on how we split our data). This can be seen by the way our test loss begins to increase and diverge from our training loss.</p>
<h4>Options for Further Investigation</h4>
<p>Here are some ideas we could try:</p>
<ul>
<li>Adding attention.</li>
<li>Adding pointers / skip connections between the encoded data and the decoder.</li>
<li>Using a more advanced sampling procedure such as beam search.</li>
<li>Using GloVe embeddings and a shared embedding layer (similar to the Ludwig model).</li>
<li>Adding a discriminator to the output (similar to the Ludwig model).</li>
<li>Using lemmas instead of words to allow an increase in useful vocabulary.</li>
<li>Use a coverage measure to prevent repetition.</li>
</ul>
<hr>
<h2>Summary</h2>
<p>In this post we have:</p>
<ul>
<li>Constructed two sequence-to-sequence models.</li>
<li>Applied theses models to our patent claim and title data.</li>
<li>Looked at the performance of each model, including identifying strengths and weaknesses.</li>
</ul>
<p>The models are still pretty bad and a long way away from a production level system.</p>
    </div>
    <hr/>
    <footer>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2018-06-21T10:02:55.991555+01:00">
          <i class="fa fa-clock-o"></i>
          Thu 21 June 2018
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="https://benhoyle.github.io/category/title-generation.html">Title Generation</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-user-o"></i>
              <a href="https://benhoyle.github.io/author/ben-hoyle.html">Ben Hoyle</a>          </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="https://benhoyle.github.io/tag/spot_checking.html">#spot_checking</a>          </li>
      </ul>
    </footer>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <p class="col-sm-6 text-sm-left">
    <a href="https://www.linkedin.com/in/benhoyle/" class="text-muted" target="_blank">Ben Hoyle</a> - <a href="https://twitter.com/bjh_ip" ><i class="fab fa-twitter"></i></a>
  </p>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" class="text-muted" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" class="text-muted" target="_blank">Adapted from &#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$( document ).ready(function() {
    $('table').addClass('table table-bordered');
    $('tbody').addClass('table-striped');
    $('th').addClass('text-center');
});
</script></html>