<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Machine Learning Projects - Title Generation</title><link href="https://benhoyle.github.io/" rel="alternate"></link><link href="https://benhoyle.github.io/feeds/title-generation.atom.xml" rel="self"></link><id>https://benhoyle.github.io/</id><updated>2018-04-01T13:34:22+01:00</updated><entry><title>1. Title Generation - Defining the Problem</title><link href="https://benhoyle.github.io/part-1-title-generation-defining-the-problem.html" rel="alternate"></link><published>2018-04-01T13:34:22+01:00</published><updated>2018-04-01T13:34:22+01:00</updated><author><name>Ben Hoyle</name></author><id>tag:benhoyle.github.io,2018-04-01:/part-1-title-generation-defining-the-problem.html</id><summary type="html">&lt;p&gt;This post looks at how we define the problem of title generation.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;1. Title Generation - Defining the Problem&lt;/h1&gt;
&lt;p&gt;This post looks at how we define the problem of title generation.  &lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;What is the problem?&lt;/h1&gt;
&lt;h2&gt;Problem Introduction&lt;/h2&gt;
&lt;p&gt;Every patent application has a title. This is based on the subject matter of the application.&lt;/p&gt;
&lt;p&gt;The scope of each patent application is defined by the patent claims. These are typically set out at the end of the patent specification. The broadest scope of a patent application is defined by the so-called "independent claims". These are claims that do not reference any other claims. Claim 1 of each patent application is typically the first independent claim.&lt;/p&gt;
&lt;p&gt;Our machine learning problem then becomes: can we generate a title given the main patent claim?&lt;/p&gt;
&lt;h2&gt;Formalised&lt;/h2&gt;
&lt;p&gt;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&lt;/p&gt;
&lt;p&gt;Here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;T = generate a title.&lt;/li&gt;
&lt;li&gt;E = a corpus of US Patent Publications that include title and claim text.&lt;/li&gt;
&lt;li&gt;P = more difficult - let's look at this in more detail below. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Classification problems have a simple performance measure: classification accuracy. Here we are trying to generate a string of text, and we have a "ground truth" or original string of text to compare it too. &lt;/p&gt;
&lt;p&gt;One performance measure we could look at is a string difference or distance metric. There are several of these, a common one being the &lt;a href="https://en.m.wikipedia.org/wiki/Levenshtein_distance"&gt;Levenshtein distance&lt;/a&gt;. One issue with many of these metrics is they heavily penalise different acceptable versions of a word sequence. For example, changing word order may still provide an acceptable output but may have a very high string difference metric value. Also synonyms would likewise lead to a high string distance metric value but may produce an acceptable answer.&lt;/p&gt;
&lt;p&gt;For the similar problem of neural machine translation, the &lt;a href="https://en.m.wikipedia.org/wiki/BLEU"&gt;BLEU&lt;/a&gt; metric is often used as a performance measure. It could be investigated whether this metric could be used in the present case.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.m.wikipedia.org/wiki/Perplexity"&gt;Perplexity&lt;/a&gt; is another metric that is often used to evaluate text generation models.&lt;/p&gt;
&lt;p&gt;The ideal performance measure would be a human score of the generated sentence indicating its suitability. This though is expensive in terms of human effort and is highly impractical for use in model training. There is though the potential to use an actor-critic or &lt;a href="https://en.m.wikipedia.org/wiki/Generative_adversarial_network"&gt;Generative Adversarial Network&lt;/a&gt;. This architecture uses an additional binary classifier that attempts to predict whether a generated text string came from the set of original text strings or not. Its loss can be used in the loss function of the machine learning model.&lt;/p&gt;
&lt;h2&gt;Assumptions&lt;/h2&gt;
&lt;p&gt;Below are some assumptions that may apply to the problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We assume the content of a patent application may be represented by its first independent claim.&lt;/li&gt;
&lt;li&gt;It is an open question as to whether we could get better results using the full text of the detailed description. However, the first claim of an application gives us a more manageable set of text to work with.&lt;/li&gt;
&lt;li&gt;We assume that there are some underlying patterns to title formation that can be learnt by a machine learning model.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2&gt;Similar Problems&lt;/h2&gt;
&lt;h3&gt;Text Summarization&lt;/h3&gt;
&lt;p&gt;The current problem is similar to the problem of text summarization, i.e. generating a sentence that summarizes a block of larger text.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://rare-technologies.com/text-summarization-in-python-extractive-vs-abstractive-techniques-revisited/"&gt;This is a useful post from students at Rare Technologies&lt;/a&gt;. It looks at the different metrics that may be used and shows some issues that may result from more complex neural models.&lt;/p&gt;
&lt;p&gt;Most state of the art models for text summarization appear to be based on a combination of a sequence-to-sequence model (also referred to as  encoder-decoder model) and an attention mechanism.&lt;/p&gt;
&lt;p&gt;Jason Brownlee at Machine Learning Mastery has a &lt;a href="https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/"&gt;post&lt;/a&gt; detailing a simple encoder-decoder model for text summarization in Keras.&lt;/p&gt;
&lt;p&gt;Abigail See has a great post &lt;a href="http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html"&gt;here&lt;/a&gt; that explains a state of the art text summarization system. As well as using a sequence-to-sequence model with attention, Abigail also adds pointers and coverage to solve the problems of factual inaccuracies and repetition.&lt;/p&gt;
&lt;h3&gt;Neural Machine Translation&lt;/h3&gt;
&lt;p&gt;Text summarization is similar to the problem of neural machine translation. There is a larger body of literature on neural machine translation as it is arguably an easier problem to solve: typically you have pairs of input and output sentences in different languages, where there may be similarities in syntax and word order between the input and the output.&lt;/p&gt;
&lt;p&gt;Like text summarization, the state of the art models use sequence-to-sequence architectures with attention. As the body of literature is larger for neural machine translation, it may be useful to complement the literature on text summarization.&lt;/p&gt;
&lt;p&gt;In terms of practical implementations, there is a detailed tutorial on building a state-of-the-art neural machine translation system in Tensorflow &lt;a href="https://www.tensorflow.org/tutorials/seq2seq"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;Why does the problem need to be solved?&lt;/h1&gt;
&lt;p&gt;Generative text models have great promise for automating many tasks. This toy problem will introduce us to many of the aspects (and problems) of more complex text generation systems.&lt;/p&gt;
&lt;p&gt;By solving the problem of title generation, we can then move on to bigger problems, such as generating larger blocks of text.&lt;/p&gt;
&lt;h3&gt;My Motivation&lt;/h3&gt;
&lt;p&gt;Text generation is a hard problem in the field of natural language processing. It is an ongoing research area. By addressing a small toy problem for which I have data I can better understand the mechanisms and problems in the field. I can also build a collection of tools and methodologies I can use as a starting point for more complex text generation.&lt;/p&gt;
&lt;p&gt;By exploring the data I can also get a feel for the syntactic and semantic structure of patent text.&lt;/p&gt;
&lt;h3&gt;Solution Benefits&lt;/h3&gt;
&lt;p&gt;The solution could be used to automatically generate a patent application title based on the claims.&lt;/p&gt;
&lt;p&gt;As the title is short this has limited benefit for reducing drafting time. However, it could be useful for determining keywords or a lower dimensionality claim representation for search. For example, to find prior art, search may be performed in a title space rather than a claim space.&lt;/p&gt;
&lt;h3&gt;Solution Use&lt;/h3&gt;
&lt;p&gt;I will write up my results in a blog post report. There will be a separate blog post for each stage in my investigation and a summary page reporting my results.&lt;/p&gt;
&lt;p&gt;The code from the solution may be used in my patentdata project, for example, claim-to-title conversion may be added to patentdata functionality. This could be implemented as a "title()" method on a Claim object. &lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;How Would I Solve the Problem?&lt;/h1&gt;
&lt;p&gt;It is worth considering how the problem may be solved manually. &lt;/p&gt;
&lt;p&gt;The guidance on drafting patent specification titles is that the title should not be more limiting that the broadest independent claim. In practical terms, this means that the title should typically only contain words that appear in the main claim. This is a nice limitation for our current problem: the text of the main claim (e.g. claim 1) should ideally provide all the information we need to generate a title.&lt;/p&gt;
&lt;p&gt;There are patterns in how titles are drafted. Different patent attorneys follow different conventions. Here are a few widely seen patterns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The title is taken from the preamble of a main claim. The preamble is the section of a claim before the phrase "comprising:". For example, a claim may be "A method of painting a house comprising: exploding a paint can." and the preamble is then "a method of painting a house" or even "painting a house". This could be used directly as the title.&lt;/li&gt;
&lt;li&gt;The title features the categories of all the independent claims. For example, if the independent claims relate to a "method", a "system" and a "computer program", the title may be "Method, System and Computer Program [for doing X]". This could cause a problem with our approach as if we use only the text of claim 1, we do not have the categories of the other independent claims. This could be remedied by adding the categories as additional data or using the text of all the independent claims. However, this would be at the cost of increasing the size of our input data. It might be possible for our model to learn the categories that regularly appear and then "guess" these.&lt;/li&gt;
&lt;li&gt;Another pattern is to take the core feature of the independent claim and abstract it. For example, if the new and novel feature of our claim related to a "handle for a spade", where the claim was something like "A spade comprising: a handle with a new widget.", then the title may be "HANDLE FOR A SPADE" or "SPADE HANDLE".&lt;/li&gt;
&lt;/ul&gt;</content><category term="defining_the_problem"></category></entry></feed>