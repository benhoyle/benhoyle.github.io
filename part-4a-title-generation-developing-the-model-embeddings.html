<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  4A. Title Generation - Developing the Model - Embeddings | Practical Machine Learning Adventures
</title>
  <link rel="canonical" href="https://benhoyle.github.io/part-4a-title-generation-developing-the-model-embeddings.html">


  <link rel="stylesheet" href="https://benhoyle.github.io/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.9/css/all.css" integrity="sha384-5SOiIsAziJl6AWe0HWRKTXlfcSHKmYV4RBF18PPJ173Kzn7jzMyFuTtk8JA7QQG1" crossorigin="anonymous">
  <link rel="stylesheet" href="https://benhoyle.github.io/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://benhoyle.github.io/theme/css/theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://benhoyle.github.io/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://benhoyle.github.io/feeds/title-generation.atom.xml">  
  <meta name="description" content="This post looks at developing our initial models to include state of the art features to improve results.">
  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o);
      a.async = 1;
      a.src = g;
      m = s.getElementsByTagName(o)[0];
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-96290248-2', 'auto');
    ga('send', 'pageview');
  </script>


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="https://benhoyle.github.io/">Practical Machine Learning Adventures</a></h1>
      <p class="text-muted">A selection of machine learning projects</p>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
  <article class="article">
    <div class="content">
      <h1>4A. Title Generation - Developing the Model - Embeddings</h1>
<p>This post looks at developing our initial models to include state of the art features to improve results.</p>
<p>To recap:</p>
<ul>
<li>We have two models: the Ludwig model and the Chollet/Brownlee model. </li>
<li>Performance so far has been fairly poor.</li>
<li>Each model had slightly different characteristics - the Ludwig model produced better formed output but seemed to simply memorise and repeat titles, the Chollet/Brownlee model had a lower loss and appeared to memorise less but produced more nonsensical outputs.</li>
</ul>
<p>In our last post we identified a number of ways to improve our models:</p>
<ol>
<li>Use GloVe encodings and a shared embedding layer. </li>
<li>Add attention.</li>
<li>Add pointers / skip connections between our input and our output.</li>
<li>Use a coverage measure.</li>
<li>Use different word forms such as lemmas or stems.</li>
<li>Use a GAN-style discriminator on the output.</li>
<li>Improve our sampling by employing beam search.</li>
</ol>
<p>Over the next set of posts, we will look at the first two of these in detail. This post starts with using pre-trained embedding weights and an embedding layer that is shared across the encoder and decoder input.</p>
<div class="highlight"><pre><span></span><span class="c1"># Imports </span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
</pre></div>


<h2>Using Pre-trained Shared Embeddings</h2>
<p>Here we will look at two complementary modifications to our model:</p>
<ul>
<li>We can speed-up our training by using a set of pre-trained embedding weights; and</li>
<li>As the encoder and decoder both take words with a similar vocabulary as input, we can share a single set of weights between the encoder and decoder.</li>
</ul>
<h3>Pre-trained Embedding Weights</h3>
<p>There are several different ways to generate and use a set of pre-trained word embeddings. Two very common approaches are to use either pretrained <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> or <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a> embeddings. Anecdotally, for small toy projects, both have similar characteristics. The GloVe vectors are the easiest to download and use, so we'll start with those.  </p>
<p>As a start, I recommend using the 100 dimension vectors generated from a crawl of 6 billion words. This can be downloaded as a text file from <a href="http://nlp.stanford.edu/data/glove.6B.zip">here</a>. This is to be placed in a <code>/glove</code> directory.</p>
<p>Then we follow the steps from Ludwig's example to generate our embedding matrix. </p>
<p>The code below reads the entries in the text file and generates a dictionary indexed by a word with a numpy array containing the embedding vector as an entry.</p>
<div class="highlight"><pre><span></span><span class="n">GLOVE_DIR</span> <span class="o">=</span> <span class="s2">&quot;glove/&quot;</span>

<span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c1"># For Python 3 tweaked to add &#39;rb&#39;</span>
<span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">),</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="c1"># Tweaked to decode the binary text values</span>
    <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
    <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>400000
</pre></div>


<div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">embeddings_index</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;irati&#39;,
 &#39;fotiou&#39;,
 &#39;8-year&#39;,
 &#39;usagi&#39;,
 &#39;autobianchi&#39;,
 &#39;eldercare&#39;,
 &#39;puraskar&#39;,
 &#39;dench&#39;,
 &#39;ventrally&#39;,
 &#39;amsc&#39;]
</pre></div>


<p>As we can see there are 400,000 words. Some of which are fairly arcane (note "eldercare", which is a US-centric term). Below is the 100d word embedding vector for "the".</p>
<div class="highlight"><pre><span></span><span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;the&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,
       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,
        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,
       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,
        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,
       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,
        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,
        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,
       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,
       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,
       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,
       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,
       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,
       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,
       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,
        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,
       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)
</pre></div>


<hr>
<h3>Loading and Tokenizing Data</h3>
<p>Initially we load our data as before. Our tokenizing routine changes a little, as we will use a common tokenizer on both the claim text and title. This allows us to use a shared embedding layer.</p>
<div class="highlight"><pre><span></span><span class="c1"># Set parameters</span>
<span class="n">num_decoder_tokens</span> <span class="o">=</span> <span class="mi">2500</span> <span class="c1"># This is our output title vocabulary</span>
<span class="n">num_encoder_tokens</span> <span class="o">=</span> <span class="mi">2500</span> <span class="c1"># This is our input claim vocabulary</span>
<span class="n">encoder_seq_length</span> <span class="o">=</span> <span class="mi">300</span> <span class="c1"># This is our limit for our input claim length</span>
<span class="n">decoder_seq_length</span> <span class="o">=</span> <span class="mi">22</span> <span class="c1"># This is our limit for our output title length - 20 + 2 for start/stop</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">PIK</span> <span class="o">=</span> <span class="s2">&quot;claim_and_title.data&quot;</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">PIK</span><span class="p">):</span>
    <span class="c1"># Download file</span>
    <span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">benhoyle</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">notebooks</span><span class="o">/</span><span class="n">title_generation</span><span class="o">/</span><span class="n">claim_and_title</span><span class="o">.</span><span class="n">data</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">PIK</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Loading data&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;{0} samples loaded&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">Adding start and stop tokens to output&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="n">c</span><span class="p">,</span> <span class="s2">&quot;startseq {0} stopseq&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">An example title:&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;----&quot;</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;An example claim:&quot;</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>Loading data
30000 samples loaded


Adding start and stop tokens to output


An example title: startseq System and method for session restoration at geo-redundant gateways stopseq
----
An example claim: 
1. A method for managing a backup service gateway (SGW) associated with a primary SGW, the method comprising:
periodically receiving from the primary SGW at least a portion of corresponding UE session state information, the received portion of session state information being sufficient to enable the backup SGW to indicate to an inquiring management entity that UEs having an active session supported by the primary SGW are in a live state; and
in response to a failure of the primary SGW, the backup SGW assuming management of IP addresses and paths associated with said primary SGW and transmitting a Downlink Data Notification (DDN) toward a Mobility Management Entity (MME) for each of said UEs having an active session supported by the failed primary SGW to detach from the network and reattach to the network, wherein each DDN causes the MME to send a detach request with a reattach request code to the respective UE.
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.preprocessing</span> <span class="kn">import</span> <span class="n">text</span>
<span class="n">t_joint</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">Tokenizer</span><span class="p">(</span>
                <span class="n">num_words</span><span class="o">=</span><span class="n">num_encoder_tokens</span><span class="p">,</span> 
                <span class="n">lower</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">char_level</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                <span class="n">oov_token</span><span class="o">=</span><span class="s2">&quot;&lt;UNK&gt;&quot;</span>
<span class="p">)</span>
<span class="n">X_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
<span class="n">Y_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">data</span><span class="p">]</span>
<span class="n">total_texts</span> <span class="o">=</span> <span class="n">X_texts</span> <span class="o">+</span> <span class="n">Y_texts</span>
<span class="n">t_joint</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">total_texts</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">local</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="o">.</span><span class="mi">5</span><span class="o">/</span><span class="n">dist</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">h5py</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">36</span><span class="p">:</span> <span class="ne">FutureWarning</span><span class="p">:</span> <span class="n">Conversion</span> <span class="n">of</span> <span class="n">the</span> <span class="n">second</span> <span class="n">argument</span> <span class="n">of</span> <span class="n">issubdtype</span> <span class="kn">from</span> <span class="sb">`float`</span> <span class="n">to</span> <span class="sb">`np.floating`</span> <span class="ow">is</span> <span class="n">deprecated</span><span class="o">.</span> <span class="n">In</span> <span class="n">future</span><span class="p">,</span> <span class="n">it</span> <span class="n">will</span> <span class="n">be</span> <span class="n">treated</span> <span class="k">as</span> <span class="sb">`np.float64 == np.dtype(float).type`</span><span class="o">.</span>
  <span class="kn">from</span> <span class="nn">._conv</span> <span class="kn">import</span> <span class="n">register_converters</span> <span class="k">as</span> <span class="n">_register_converters</span>
<span class="n">Using</span> <span class="n">TensorFlow</span> <span class="n">backend</span><span class="o">.</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="nb">list</span><span class="p">(</span><span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>[&#39;remapper&#39;,
 &#39;inactive&#39;,
 &#39;imposes&#39;,
 &#39;overestimates&#39;,
 &#39;roman&#39;,
 &#39;mitigating&#39;,
 &quot;location&#39;s&quot;,
 &#39;56a&#39;,
 &#39;buckle&#39;,
 &#39;billable&#39;]
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_seqs</span> <span class="o">=</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">X_texts</span><span class="p">)</span>
<span class="n">Y_seqs</span> <span class="o">=</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">Y_texts</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our input sequences (claims) have a max integer value of {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">max</span><span class="p">([</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_seqs</span><span class="p">])))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our output sequences (titles) have a max integer value of {0}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">max</span><span class="p">([</span><span class="nb">max</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y_seqs</span><span class="p">])))</span>
</pre></div>


<div class="highlight"><pre><span></span>Our input sequences (claims) have a max integer value of 2499
Our output sequences (titles) have a vocabulary of 2499 words
</pre></div>


<div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="nb">max</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_seqs</span><span class="p">,</span> <span class="n">Y_seqs</span><span class="p">)])</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">vocab_size</span>
</pre></div>


<div class="highlight"><pre><span></span>2500
</pre></div>


<div class="highlight"><pre><span></span><span class="n">filtered_seqs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X_seqs</span><span class="p">,</span> <span class="n">Y_seqs</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">encoder_seq_length</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">decoder_seq_length</span><span class="p">]</span>
<span class="n">X_seqs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">filtered_seqs</span><span class="p">]</span>
<span class="n">Y_seqs</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">filtered_seqs</span><span class="p">]</span>

<span class="n">X_length</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_seqs</span><span class="p">]</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">X_length</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our longest input sequence is {0} tokens long.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>

<span class="n">Y_length</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">Y_seqs</span><span class="p">]</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Y_length</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our longest output sequence is {0} tokens long.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_length</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Our longest input sequence is 300 tokens long.
Our longest output sequence is 22 tokens long.
</pre></div>


<h3>Building the Word Embedding Matrix</h3>
<p>Now we have a dictionary of numpy arrays indexed by word strings and a dictionary of ranked words from the Keras tokenizer we can build the matrix to use as the initial weights for our shared embedding layer.</p>
<p>We need to filter the tokenizer dictionary to remove all words that are ranked less than our number of alloted tokens for the encoder and decoder inputs (e.g. all with ranks less than the vocabulary size of 2500).</p>
<p>We then iterate through our words from the tokenizer dictionary and link the word index integers with the embedding vectors.</p>
<div class="highlight"><pre><span></span><span class="n">word_embedding_size</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># As we are using the Glove 100d data</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Found {0} word vectors.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">)))</span>
<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">word_embedding_size</span><span class="p">))</span>

<span class="c1"># Filter our vocab to only the used items</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[(</span><span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">]</span>

<span class="c1"># This is from https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/      </span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</pre></div>


<div class="highlight"><pre><span></span>Found 400000 word vectors.
</pre></div>


<p>The resultant matrix should be of shape : vocabulary size by GloVe dimensions = 2500 x 100.</p>
<div class="highlight"><pre><span></span><span class="n">embedding_matrix</span><span class="o">.</span><span class="n">shape</span>
</pre></div>


<div class="highlight"><pre><span></span>(2500, 100)
</pre></div>


<h3>Finish Preparing Data</h3>
<p>Now we just finish preparing the data by padding as before.</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">X_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_seqs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">Y_texts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Y_seqs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span>1. A method for managing a backup service gateway (SGW) associated with a primary SGW, the method comprising:
periodically receiving from the primary SGW at least a portion of corresponding UE session state information, the received portion of session state information being sufficient to enable the backup SGW to indicate to an inquiring management entity that UEs having an active session supported by the primary SGW are in a live state; and
in response to a failure of the primary SGW, the backup SGW assuming management of IP addresses and paths associated with said primary SGW and transmitting a Downlink Data Notification (DDN) toward a Mobility Management Entity (MME) for each of said UEs having an active session supported by the failed primary SGW to detach from the network and reattach to the network, wherein each DDN causes the MME to send a detach request with a reattach request code to the respective UE.

 [31, 2, 29, 8, 448, 2, 552, 91, 1047, 42, 19, 2, 397, 1, 29, 26, 1959, 51, 20, 1, 397, 14, 25, 2, 74, 3, 61, 352, 109, 28, 1, 96, 74, 3, 352, 109, 28, 58, 2214, 4, 782, 1, 552, 4, 1008, 4, 11, 142, 261, 24, 69, 11, 500, 352, 1510, 15, 1, 397, 60, 6, 2, 1672, 109, 5, 6, 75, 4, 2, 888, 3, 1, 397, 1, 552, 142, 3, 603, 685, 5, 937, 42, 19, 16, 397, 5, 252, 2, 9, 494, 1653, 2, 142, 261, 8, 32, 3, 16, 69, 11, 500, 352, 1510, 15, 1, 2193, 397, 4, 20, 1, 53, 5, 4, 1, 53, 18, 32, 957, 1, 4, 742, 2, 65, 19, 2, 65, 97, 4, 1, 118]
startseq System and method for session restoration at geo-redundant gateways stopseq [34, 30, 5, 29, 8, 352, 14, 1836, 35]
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Pad the data</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X_seqs</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">encoder_seq_length</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">Y_seqs</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">decoder_seq_length</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;post&#39;</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Our X data has shape {0} and our Y data has shape {1}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Our</span> <span class="o">X</span> <span class="n">data</span> <span class="k">has</span> <span class="nb">shape</span> (<span class="mi">25632</span>, <span class="mi">300</span>) <span class="o">and</span> <span class="k">our</span> <span class="n">Y</span> <span class="n">data</span> <span class="k">has</span> <span class="nb">shape</span> (<span class="mi">25632</span>, <span class="mi">22</span>)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>array([  34,   30,    5,   29,    8,  352,   14, 1836,   35,    0,    0,
          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
      dtype=int32)
</pre></div>


<hr>
<h2>Adapting the Ludwig Model</h2>
<p>We will now adapt the Ludwig model used in the previous post to use a shared embedding layer and the pre-trained GloVe vectors.</p>
<p>This is fairly simple - we just define an embedding layer and initialise it with the generated embedding matrix. We then specify that it is to be used on both the encoder and decoder inputs.</p>
<div class="highlight"><pre><span></span><span class="c1"># imports</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">concatenate</span>

<span class="n">y_vocab_len</span> <span class="o">=</span> <span class="n">num_decoder_tokens</span> <span class="c1"># This is our output title vocabulary</span>
<span class="n">X_vocab_len</span> <span class="o">=</span> <span class="n">num_encoder_tokens</span> <span class="c1"># This is our input claim vocabulary</span>
<span class="n">X_max_len</span> <span class="o">=</span> <span class="n">encoder_seq_length</span> <span class="c1"># This is our limit for our input claim length</span>
<span class="n">y_max_len</span> <span class="o">=</span> <span class="n">decoder_seq_length</span> <span class="c1"># This is our limit for our output title length - 20 + 2 for start/stop</span>

<span class="c1"># source text input model</span>
<span class="n">inputs1</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_max_len</span><span class="p">,))</span>
<span class="c1">#am1 = Embedding(X_vocab_len, 128)(inputs1)</span>
<span class="n">Shared_Embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span>
    <span class="n">output_dim</span><span class="o">=</span><span class="n">word_embedding_size</span><span class="p">,</span> 
    <span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> 
    <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">embedding_matrix</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">am1</span> <span class="o">=</span> <span class="n">Shared_Embedding</span><span class="p">(</span><span class="n">inputs1</span><span class="p">)</span>
<span class="n">am2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">)(</span><span class="n">am1</span><span class="p">)</span>
<span class="c1"># summary input model</span>
<span class="n">inputs2</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">y_max_len</span><span class="p">,))</span>
<span class="n">sm1</span> <span class="o">=</span> <span class="n">Shared_Embedding</span><span class="p">(</span><span class="n">inputs2</span><span class="p">)</span>
<span class="n">sm2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">)(</span><span class="n">sm1</span><span class="p">)</span>
<span class="c1"># decoder output model</span>
<span class="n">decoder1</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">([</span><span class="n">am2</span><span class="p">,</span> <span class="n">sm2</span><span class="p">])</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">y_vocab_len</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">decoder1</span><span class="p">)</span>
<span class="c1"># tie it together [article, summary] [word]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">inputs1</span><span class="p">,</span> <span class="n">inputs2</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_7 (InputLayer)            (None, 300)          0                                            
__________________________________________________________________________________________________
input_8 (InputLayer)            (None, 22)           0                                            
__________________________________________________________________________________________________
embedding_3 (Embedding)         (None, 2500, 100)    250000      input_7[0][0]                    
                                                                 input_8[0][0]                    
__________________________________________________________________________________________________
lstm_5 (LSTM)                   (None, 128)          117248      embedding_3[0][0]                
__________________________________________________________________________________________________
lstm_6 (LSTM)                   (None, 128)          117248      embedding_3[1][0]                
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 256)          0           lstm_5[0][0]                     
                                                                 lstm_6[0][0]                     
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 2500)         642500      concatenate_2[0][0]              
==================================================================================================
Total params: 1,126,996
Trainable params: 1,126,996
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>


<p>Now we see that the embedding is shared by both LSTMs. </p>
<p>We can now build our model in pretty much the same way as before...</p>
<div class="highlight"><pre><span></span><span class="c1"># We need to split into train and test data</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># seed for reproducing same results</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># split the data into training (80%) and testing (20%)</span>
<span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Y</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>array([  34,   30,    5,   29,    8,  352,   14, 1836,   35,    0,    0,
          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],
      dtype=int32)
</pre></div>


<p>One small bug we had to fix is that we can now no longer hard-code the integer indexes for our start and stop control tokens. We thus use the word_index dictionary to look up the integer based on the text.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_set</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Generate the data for training/validation from X and Y.</span>
<span class="sd">    i_end is the end of the set, i is the start.&quot;&quot;&quot;</span>
    <span class="n">set_size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">limit_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i_end</span><span class="p">]:</span>
        <span class="c1"># Edited below to use integer value of EOS symbol</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">sent</span><span class="o">==</span><span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;stopseq&quot;</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>  <span class="c1">#  the position of the symbol EOS</span>
        <span class="n">set_size</span> <span class="o">+=</span> <span class="n">limit</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">limit_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">limit</span><span class="p">)</span>

    <span class="c1"># We need to change this bit to set our array size based on the limit values</span>
    <span class="c1"># Generate blank arrays for the set</span>
    <span class="n">I_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">set_size</span><span class="p">,</span> <span class="n">X_max_len</span><span class="p">))</span>
    <span class="n">I_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">set_size</span><span class="p">,</span> <span class="n">y_max_len</span><span class="p">))</span>
    <span class="c1"># This below is a big array</span>
    <span class="n">Y_set</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">set_size</span><span class="p">,</span> <span class="n">y_vocab_len</span><span class="p">))</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Now we want to create, for each sample, a set of examples for each word in the title</span>
    <span class="c1"># Have we just been training on 0 to 100?!?!</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">i_end</span> <span class="o">-</span> <span class="n">i</span><span class="p">)):</span>
        <span class="c1"># for each X and y in set of NB_SET </span>

        <span class="c1"># We need to build the input for the second encoder for the next word in y</span>
        <span class="c1"># I.e. for word 3 in the title the input2 consists of words 1 and 2 (using teacher forcing)</span>

        <span class="c1"># Get length of current title - i.e. where the integer = 2 = stopseq</span>
        <span class="n">limit</span> <span class="o">=</span> <span class="n">limit_list</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>

        <span class="c1"># We only need to create examples up to the length of the title </span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">limit</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>

            <span class="c1"># Generate our one-hot y out</span>
            <span class="n">one_hot_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_vocab_len</span><span class="p">))</span>
            <span class="c1"># This builds our one-hot generation into our training loop</span>
            <span class="c1"># The l and m respectively iterate through the samples and the output sequence elements</span>
            <span class="n">one_hot_out</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">Y</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">i</span><span class="p">][</span><span class="n">m</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># Create a blank row/array for a partial input for our summary model - this is fed into the decoder</span>
            <span class="c1"># It is of the same size as our title</span>
            <span class="n">partial_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_max_len</span><span class="p">))</span>
            <span class="c1"># Don&#39;t we also need to set partial input [0] to startseq as well? - no that&#39;s taken care of</span>
            <span class="c1"># by m starting at one but our range below starting at 0</span>

            <span class="c1"># Because we are zero padding add words up to m to end - DOES THIS STILL WORK IF WE ZERO PAD</span>
            <span class="c1"># AT THE END? - Yes but we just feed the words with zeros first?</span>
            <span class="c1"># What happens if we change this to 0:m?! - if we have [1, 2, 3, 4] this will generate</span>
            <span class="c1"># [0,0,0,1], [0,0,1,2], [0,1, 2, 3]</span>
            <span class="c1"># Our zero padding is at the end though so our seqs looks like [1,2,3,0,0,0], </span>
            <span class="c1"># But I know you want the data need the end of the input seq to prevent forgetting</span>
            <span class="n">partial_input</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">m</span><span class="p">:]</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">]</span>

            <span class="c1"># This fills in each sample of the training data, i.e. count increments up to set size</span>
            <span class="n">I_1</span><span class="p">[</span><span class="n">count</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="n">i</span><span class="p">]</span>
            <span class="n">I_2</span><span class="p">[</span><span class="n">count</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">partial_input</span>
            <span class="n">Y_set</span><span class="p">[</span><span class="n">count</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">one_hot_out</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Shuffle the I_1, I_2 and Y_set vectors for better training - trick from RL</span>
        <span class="c1"># - see here - np.take(X,np.random.permutation(X.shape[0]),axis=0,out=X);</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">I_1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">I_1</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">I_1</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">I_2</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">I_2</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">Y_set</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">Y_set</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">I_1</span><span class="p">,</span> <span class="n">I_2</span><span class="p">,</span> <span class="n">Y_set</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Basing training in sets code on here - https://github.com/ChunML/seq2seq/blob/master/seq2seq.py</span>

<span class="c1"># Function to look for saved weights file</span>
<span class="k">def</span> <span class="nf">find_checkpoint_file</span><span class="p">(</span><span class="n">folder</span><span class="p">):</span>
    <span class="n">checkpoint_file</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;v2_kerascheckpoint&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">checkpoint_file</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[]</span>
    <span class="n">modified_time</span> <span class="o">=</span> <span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">getmtime</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">checkpoint_file</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">checkpoint_file</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">modified_time</span><span class="p">)]</span>

<span class="c1"># Finding trained weights of previous epoch if any</span>
<span class="n">saved_weights</span> <span class="o">=</span> <span class="n">find_checkpoint_file</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="n">k_start</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># If any trained weight was found, then load them into the model</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[INFO] Saved weights found, loading...&#39;</span><span class="p">)</span>
    <span class="n">epoch</span> <span class="o">=</span> <span class="n">saved_weights</span><span class="p">[</span><span class="n">saved_weights</span><span class="o">.</span><span class="n">rfind</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">saved_weights</span><span class="o">.</span><span class="n">rfind</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">load_weights</span><span class="p">(</span><span class="n">saved_weights</span><span class="p">)</span>
    <span class="n">k_start</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># So instead of X we have [inputs1, inputs2] - this is where we need to fold in </span>
<span class="c1"># - https://github.com/oswaldoludwig/Seq2seq-Chatbot-for-Keras/blob/master/train_bot.py</span>

<span class="c1"># So we have inputs2 that build up - we have a set of inputs2 up to the length of inputs2</span>

<span class="c1"># We need to refactor some of the loops below as functions - we can then apply to test data to generate a validation set</span>
</pre></div>


<div class="highlight"><pre><span></span>[INFO] Saved weights found, loading...
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span> <span class="c1"># Depends on GPU - most values are around this 32-128 </span>
<span class="n">NB_EPOCH</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1"># Number of examples to group together in a set - 100 is fast / 1000 is too much on an 8-core i7 laptop</span>
<span class="c1"># I think 100 is good - 250 takes a time to generate the sets of test data</span>
<span class="n">NB_SET</span> <span class="o">=</span> <span class="mi">250</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">i_end</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">num_test</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="c1"># Initialise history of accuracy</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Continue from loaded epoch number or new epoch if not loaded</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k_start</span><span class="p">,</span> <span class="n">NB_EPOCH</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Shuffling the training data every epoch to avoid local minima</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_examples</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">Y_train</span> <span class="o">=</span> <span class="n">Y_train</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>

    <span class="c1"># This for loop rotates through NB_SET samples at a time to avoid memory issues</span>
    <span class="c1"># E.g. Training 100 sequences at a time</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">,</span> <span class="n">NB_SET</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">NB_SET</span> <span class="o">&gt;=</span> <span class="n">num_examples</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">num_examples</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">NB_SET</span>

        <span class="c1"># Generate a range for the test data</span>
        <span class="n">i_test</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>
        <span class="n">i_test_end</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i_end</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>

        <span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">,</span> <span class="n">Y_set_train</span> <span class="o">=</span> <span class="n">generate_set</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">,</span> <span class="n">Y_set_test</span> <span class="o">=</span> <span class="n">generate_set</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">i_test_end</span><span class="p">,</span> <span class="n">i_test</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[INFO] Training model: epoch {} - {}/{} samples&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">))</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="p">[</span><span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">],</span> 
            <span class="n">Y_set_train</span><span class="p">,</span> 
            <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">],</span> <span class="n">Y_set_test</span><span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> 
            <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
        <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
        <span class="c1"># Get history and apppend new data to running set here</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s1">&#39;v2_kerascheckpoint_epoch_{}.hdf5&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>[INFO] Training model: epoch 2 - 0/20505 samples
Train on 2534 samples, validate on 603 samples
Epoch 1/1
2534/2534 [==============================] - 22s 9ms/step - loss: 2.2101 - val_loss: 2.3826
[INFO] Training model: epoch 2 - 250/20505 samples
Train on 2464 samples, validate on 677 samples
Epoch 1/1
2464/2464 [==============================] - 22s 9ms/step - loss: 2.1098 - val_loss: 2.2632
[INFO] Training model: epoch 2 - 500/20505 samples
Train on 2404 samples, validate on 630 samples
Epoch 1/1
2404/2404 [==============================] - 21s 9ms/step - loss: 2.1046 - val_loss: 2.4288

....YADAYADAYADA...

[INFO] Training model: epoch 20 - 20000/20505 samples
Train on 2452 samples, validate on 662 samples
Epoch 1/1
2452/2452 [==============================] - 23s 9ms/step - loss: 1.1216 - val_loss: 2.6810
[INFO] Training model: epoch 20 - 20250/20505 samples
Train on 2506 samples, validate on 607 samples
Epoch 1/1
2506/2506 [==============================] - 24s 10ms/step - loss: 1.1131 - val_loss: 2.7156
[INFO] Training model: epoch 20 - 20500/20505 samples
Train on 39 samples, validate on 22 samples
Epoch 1/1
39/39 [==============================] - 1s 13ms/step - loss: 0.8315 - val_loss: 3.1535
</pre></div>


<div class="highlight"><pre><span></span>So using the shared embedding appears to provide a small advantage in reducing our loss function. It also lowers the number of parameters for our model, which is a good thing as it reduces over-fitting and training time.


```python
# summarize history for accuracy
plt.plot(train_loss)
plt.plot(val_loss)
plt.title(&#39;Model Loss&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.xlabel(&#39;Iteration&#39;)
plt.legend([&#39;train&#39;, &#39;test&#39;], loc=&#39;upper left&#39;)
plt.show()
</pre></div>


<p><img alt="png" src="https://benhoyle.github.io/images/TG_4A_output_44_0.png"></p>
<p>Now we've trained our model we need some code to test it. Again, this is pretty much as before but with some tweaks to use the word_index dictionaries to look up our control characters.</p>
<div class="highlight"><pre><span></span><span class="c1"># Set up dictionary to translate indices to words</span>
<span class="n">y_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">char</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">)</span>

<span class="n">x_dictionary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">char</span><span class="p">)</span> <span class="k">for</span> <span class="n">char</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">)</span>

<span class="k">def</span> <span class="nf">seq2text</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">dictionary</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="c1"># Adapted to take account of different control integers</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;stopseq&quot;</span><span class="p">],</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;startseq&quot;</span><span class="p">],</span> <span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dictionary</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">dictionary</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="n">text</span> <span class="o">=</span> <span class="n">text</span> <span class="o">+</span> <span class="n">w</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span>
    <span class="k">return</span> <span class="n">text</span>

<span class="k">def</span> <span class="nf">greedy_decoder</span><span class="p">(</span><span class="n">X_seq</span><span class="p">):</span>
    <span class="c1"># reformat input seq</span>
    <span class="n">input_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_max_len</span><span class="p">))</span>
    <span class="n">input_seq</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">X_seq</span>
    <span class="n">flag</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">prob</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">ans_partial</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_max_len</span><span class="p">))</span>
    <span class="c1"># Add start token integer to end of ans_partial input - initially [0,0,...BOS]</span>
    <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;startseq&quot;</span><span class="p">]</span>  <span class="c1">#  the index of the symbol BOS (begin of sentence)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_max_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ye</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">input_seq</span><span class="p">,</span> <span class="n">ans_partial</span><span class="p">])</span>
        <span class="n">yel</span> <span class="o">=</span> <span class="n">ye</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">yel</span><span class="p">)</span>
        <span class="n">mp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ye</span><span class="p">)</span>
        <span class="c1"># It is this line that sets how our training data should be arranged - need to change both</span>
        <span class="c1"># the line below shifts the existing ans_partial by 1 to the left - [0, 0, ..., BOS, 0]</span>
        <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
        <span class="c1"># This then adds the newly decoded word onto the end of ans_partial</span>
        <span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">mp</span>
        <span class="k">if</span> <span class="n">mp</span> <span class="o">==</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;stopseq&quot;</span><span class="p">]:</span>  <span class="c1">#  the index of the symbol EOS (end of sentence)</span>
            <span class="n">flag</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">flag</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>    
            <span class="n">prob</span> <span class="o">=</span> <span class="n">prob</span> <span class="o">*</span> <span class="n">p</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">ans_partial</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="k">return</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>

<span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">text</span><span class="p">,</span> <span class="n">prob</span> <span class="o">=</span> <span class="n">greedy_decoder</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {} (with prob {}). </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">prob</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: 1 a method of memory page de in a computer system comprising a plurality of virtual machine partitions managed by a hypervisor wherein each virtual machine is assigned a different dedicated memory par

Predicted title is: virtual machine using virtual machine i o port  (with prob 0.00024153314510963288). 
 Test title is: memory page de in a computer system that includes a plurality of virtual machines  
---
Sample of claim text: 1 a method for operating a server comprising a processor for automatically generating an end user interface for working with the data within a relational database defined within a relational whose dat

Predicted title is: method and system for managing database objects in a database  (with prob 0.0003222082082124349). 
 Test title is: system and method for generating automatic user interface for complex or large databases  
---
Sample of claim text: 1 a method of processing user input in a three dimensional coordinate system comprising receiving a user input of an origin reset for the three dimensional coordinate system responsive to receiving th

Predicted title is: user interface based on user interaction surfaces type sources therein  (with prob 0.0009777329678170813). 
 Test title is: three dimensional user input  
---
Sample of claim text: 1 a digital logic circuit comprising a programmable logic device configured to include a pipeline that comprises a matching stage and a downstream extension stage the matching stage being configured t

Predicted title is: system and method for performing expression based on instruction time  (with prob 9.03439755326292e-05). 
 Test title is: method and apparatus for performing similarity searching  
---
Sample of claim text: 1 a method for deriving information from a network that is used to model web traffic data the method comprising receiving by a web traffic analysis server web traffic data that is collected by a websi

Predicted title is: method and system for determining the presence of a communication network information and method of the same  (with prob 0.00038204782414073336). 
 Test title is: knowledge discovery from networks  
---
</pre></div>


<h4>Comments on Results</h4>
<p>From this training there now appears more of a link between the claim text and the predicted title. The model seems to be learning a few patterns such as - "method and system for creating a [] method and system method". </p>
<p>This result seems close:</p>
<div class="highlight"><pre><span></span>    Sample of claim text: 1 an apparatus comprising a capacitive sense array and a processing device wherein the capacitive sense array is configured to detect a presence of a touch object or a stylus wherein the capacitive se

    Predicted title is: method and apparatus for displaying a touch screen method and apparatus for the same  (with prob 1.9565373226245597e-05). 
    Test title is: capacitive sense array for detecting touch objects and an active stylus  
</pre></div>


<p>Training still appears unstable. Some of this may be due to the shuffling for regularisation.</p>
<p>The results appear an improvement though on the previous model. It is worth keeping this feature.</p>
<h4>Options for Further Investigation</h4>
<p>It may be worth experimenting with different training parameters and not shuffling the data. Lowering the batch size might reduce some of the loss variance.</p>
<p>Also adding some regularisation may help prevent overfitting. Maybe by adding some dropout (0.2?) to the LSTMs and by adding an L2 regulariser to the dense layer.</p>
<hr>
<h2>Chollet/Brownlee Model</h2>
<p>Adapting this model is also fairly straightforward. We define the shared embedding layer as before, initialised with the GloVe weights, and use in place of our separate embedding layers from before.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">randint</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">argmax</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array_equal</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>

<span class="k">def</span> <span class="nf">target_one_hot</span><span class="p">(</span><span class="n">input_seqs</span><span class="p">,</span> <span class="n">seq_max_len</span><span class="p">,</span> <span class="n">vocab_len</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Convert a sequence of integers to a one element shifted sequence of one-hot vectors.&quot;&quot;&quot;</span>
    <span class="n">one_hot_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">input_seqs</span><span class="p">),</span> <span class="n">seq_max_len</span><span class="p">,</span> <span class="n">vocab_len</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sequence</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_seqs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="n">word_int</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sequence</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">t</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Shift decoder target get so it is one ahead</span>
                <span class="n">one_hot_out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">word_int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">one_hot_out</span>

<span class="c1"># We need to convert this for our present problem - this is similar to our generate dataset above</span>
<span class="c1"># prepare data for the LSTM</span>
<span class="k">def</span> <span class="nf">get_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return encoder_input_data, decoder_input_data, and decoder_target_data, latter as one-hot&quot;&quot;&quot;</span>
    <span class="n">encoder_input_data</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i_end</span><span class="p">]</span>
    <span class="n">decoder_input_data</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i_end</span><span class="p">]</span>
    <span class="n">decoder_target_data</span> <span class="o">=</span> <span class="n">target_one_hot</span><span class="p">(</span><span class="n">decoder_input_data</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">encoder_input_data</span><span class="p">,</span> <span class="n">decoder_input_data</span><span class="p">,</span> <span class="n">decoder_target_data</span>

<span class="c1"># returns train, inference_encoder and inference_decoder models</span>
<span class="k">def</span> <span class="nf">define_models</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">):</span>
    <span class="c1"># define training encoder</span>
    <span class="c1"># Define an input sequence and process it.</span>
    <span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
    <span class="n">Shared_Embedding</span> <span class="o">=</span> <span class="n">Embedding</span><span class="p">(</span>
        <span class="n">output_dim</span><span class="o">=</span><span class="n">latent_dim</span><span class="p">,</span> 
        <span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> 
        <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">embedding_matrix</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">encoder_embedding</span> <span class="o">=</span> <span class="n">Shared_Embedding</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>
    <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)(</span><span class="n">encoder_embedding</span><span class="p">)</span>
    <span class="n">encoder_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

    <span class="c1"># Set up the decoder, using `encoder_states` as initial state.</span>
    <span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,))</span>
    <span class="c1"># Possibly share the embedding below</span>
    <span class="n">decoder_embedding</span> <span class="o">=</span> <span class="n">Shared_Embedding</span><span class="p">(</span><span class="n">decoder_inputs</span><span class="p">)</span>
    <span class="n">decoder_lstm</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder_lstm</span><span class="p">(</span><span class="n">decoder_embedding</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">encoder_states</span><span class="p">)</span>
    <span class="n">decoder_dense</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_decoder_tokens</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
    <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">)</span>

    <span class="c1"># Define the model that will turn</span>
    <span class="c1"># `encoder_input_data` &amp; `decoder_input_data` into `decoder_target_data`</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span> <span class="n">decoder_outputs</span><span class="p">)</span>

    <span class="c1"># define inference encoder</span>
    <span class="n">encoder_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">encoder_states</span><span class="p">)</span>
    <span class="c1"># define inference decoder</span>
    <span class="n">decoder_state_input_h</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
    <span class="n">decoder_state_input_c</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,))</span>
    <span class="n">decoder_states_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">decoder_state_input_h</span><span class="p">,</span> <span class="n">decoder_state_input_c</span><span class="p">]</span>
    <span class="c1"># Need to adjust this line for the embedding</span>
    <span class="n">decoder_outputs</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="n">decoder_lstm</span><span class="p">(</span><span class="n">decoder_embedding</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">decoder_states_inputs</span><span class="p">)</span>
    <span class="n">decoder_states</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>
    <span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">decoder_dense</span><span class="p">(</span><span class="n">decoder_outputs</span><span class="p">)</span>
    <span class="n">decoder_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">([</span><span class="n">decoder_inputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">decoder_states_inputs</span><span class="p">,</span> <span class="p">[</span><span class="n">decoder_outputs</span><span class="p">]</span> <span class="o">+</span> <span class="n">decoder_states</span><span class="p">)</span>
    <span class="c1"># return all models</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">encoder_model</span><span class="p">,</span> <span class="n">decoder_model</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">set_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Code to train model in sets of set_size.&quot;&quot;&quot;</span>
    <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">num_test</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Loop here to avoid memory issues with the target one hot vector</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">,</span> <span class="n">set_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">+</span> <span class="n">set_size</span> <span class="o">&gt;=</span> <span class="n">num_examples</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">num_examples</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">i_end</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">set_size</span>
        <span class="c1"># Generate a range for the test data</span>
        <span class="n">i_test</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>
        <span class="n">i_test_end</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i_end</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_test</span><span class="o">/</span><span class="n">num_examples</span><span class="p">))</span>
        <span class="c1"># Generate small sets of train and test data</span>
        <span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">,</span> <span class="n">Y_set_train</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">i_end</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
        <span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">,</span> <span class="n">Y_set_test</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">i_test</span><span class="p">,</span> <span class="n">i_test_end</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">&#39;[INFO] Training model: {}/{} samples&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">num_examples</span><span class="p">))</span>
        <span class="n">callback</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
            <span class="p">[</span><span class="n">I_1_train</span><span class="p">,</span> <span class="n">I_2_train</span><span class="p">],</span> 
            <span class="n">Y_set_train</span><span class="p">,</span> 
            <span class="n">validation_data</span><span class="o">=</span><span class="p">([</span><span class="n">I_1_test</span><span class="p">,</span> <span class="n">I_2_test</span><span class="p">],</span> <span class="n">Y_set_test</span><span class="p">),</span>
            <span class="n">batch_size</span><span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> 
            <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
        <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">callback</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span>

<span class="c1"># define model</span>
<span class="n">train</span><span class="p">,</span> <span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span> <span class="o">=</span> <span class="n">define_models</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">word_embedding_size</span><span class="p">,</span> <span class="n">embedding_matrix</span><span class="p">)</span>
<span class="n">train</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;acc&#39;</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">train</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_14 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
input_13 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
embedding_5 (Embedding)         (None, None, 100)    250000      input_13[0][0]                   
                                                                 input_14[0][0]                   
__________________________________________________________________________________________________
lstm_9 (LSTM)                   [(None, 100), (None, 80400       embedding_5[0][0]                
__________________________________________________________________________________________________
lstm_10 (LSTM)                  [(None, None, 100),  80400       embedding_5[1][0]                
                                                                 lstm_9[0][1]                     
                                                                 lstm_9[0][2]                     
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, None, 2500)   252500      lstm_10[0][0]                    
==================================================================================================
Total params: 663,300
Trainable params: 663,300
Non-trainable params: 0
__________________________________________________________________________________________________
</pre></div>


<div class="highlight"><pre><span></span><span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="p">[]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Y_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>array([ 34,  83,  87,   3, 644,  35,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0], dtype=int32)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;startseq&quot;</span><span class="p">],</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;stopseq&quot;</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span>(34, 35)
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># setup variables</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">set_size</span> <span class="o">=</span> <span class="mi">5000</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--------</span><span class="se">\n</span><span class="s2"> Epoch - &quot;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
    <span class="n">train</span><span class="p">,</span> <span class="n">tl</span><span class="p">,</span> <span class="n">vl</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">set_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">num_decoder_tokens</span><span class="p">)</span>
    <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">tl</span>
    <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">vl</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_weights</span><span class="p">(</span><span class="s2">&quot;chollet_weights_v2.hdf5&quot;</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>--------
 Epoch -  0
[INFO] Training model: 0/20505 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 77s 15ms/step - loss: 1.9163 - acc: 0.6911 - val_loss: 1.8874 - val_acc: 0.6925
[INFO] Training model: 5000/20505 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 85s 17ms/step - loss: 1.8805 - acc: 0.6966 - val_loss: 1.9176 - val_acc: 0.6897
[INFO] Training model: 10000/20505 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 85s 17ms/step - loss: 1.8370 - acc: 0.7020 - val_loss: 1.8404 - val_acc: 0.7031
--------

......etcetc....

--------
 Epoch -  19
[INFO] Training model: 0/20505 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 118s 24ms/step - loss: 1.2376 - acc: 0.7554 - val_loss: 1.5488 - val_acc: 0.7297
[INFO] Training model: 5000/20505 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 116s 23ms/step - loss: 1.2313 - acc: 0.7557 - val_loss: 1.6187 - val_acc: 0.7252
[INFO] Training model: 10000/20505 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 116s 23ms/step - loss: 1.2158 - acc: 0.7580 - val_loss: 1.5834 - val_acc: 0.7288
[INFO] Training model: 15000/20505 samples
Train on 5000 samples, validate on 1250 samples
Epoch 1/1
5000/5000 [==============================] - 118s 24ms/step - loss: 1.2380 - acc: 0.7529 - val_loss: 1.5333 - val_acc: 0.7379
[INFO] Training model: 20000/20505 samples
Train on 505 samples, validate on 127 samples
Epoch 1/1
505/505 [==============================] - 13s 25ms/step - loss: 1.2426 - acc: 0.7504 - val_loss: 1.7648 - val_acc: 0.6979
</pre></div>


<p>Again this seems to offer an improvement by lowering our loss.</p>
<div class="highlight"><pre><span></span><span class="c1"># summarize history for accuracy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="https://benhoyle.github.io/images/TG_4A_output_58_0.png"></p>
<p>Our training and model loss end up lower and are slowly decreasing. This again suggests that the shared embedding is an improvement. We still look to plateau on our training data after 40-50 iterations (~9 or 10 epochs). From about 40 iterations we start overfitting on our training data.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># helper function to sample an index from a probability array</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float64&#39;</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="n">exp_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">exp_preds</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exp_preds</span><span class="p">)</span>
    <span class="n">probas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span>
</pre></div>


<p>Again we adapt this to use the word_index entries.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">decoder_seq_length</span><span class="p">,</span> <span class="n">temp</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># encode</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">infenc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="c1"># start of sequence input</span>
    <span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;startseq&quot;</span><span class="p">]])</span>
    <span class="c1"># collect predictions</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">decoder_seq_length</span><span class="p">):</span>
        <span class="c1"># predict next char</span>
        <span class="n">yhat</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">infdec</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">target_seq</span><span class="p">]</span> <span class="o">+</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># update state</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">h</span><span class="p">,</span> <span class="n">c</span><span class="p">]</span>
        <span class="c1"># update target sequence - this needs to be the argmax</span>
        <span class="n">next_int</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">yhat</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">temp</span><span class="p">)</span>
        <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_int</span><span class="p">)</span>
        <span class="c1"># It seems like we throw a lot of information away here - can we build in the probabilities?</span>
        <span class="n">target_seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="n">next_int</span><span class="p">])</span>
        <span class="c1"># Check for stopping character</span>
        <span class="k">if</span> <span class="n">next_int</span> <span class="o">==</span> <span class="n">t_joint</span><span class="o">.</span><span class="n">word_index</span><span class="p">[</span><span class="s2">&quot;stopseq&quot;</span><span class="p">]:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Testing</span>
<span class="n">num_test_titles</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_test_titles</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">pred_seq</span> <span class="o">=</span> <span class="n">predict_sequence</span><span class="p">(</span><span class="n">infenc</span><span class="p">,</span> <span class="n">infdec</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">decoder_seq_length</span><span class="p">,</span> <span class="n">temp</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">predicted_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">pred_seq</span><span class="p">,</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">Y_test_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_dictionary</span><span class="p">)</span>
    <span class="n">claim_text</span> <span class="o">=</span> <span class="n">seq2text</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x_dictionary</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Sample of claim text: {}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">claim_text</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">]))</span>
    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Predicted title is: {}. </span><span class="se">\n</span><span class="s2"> Test title is: {} </span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">predicted_text</span><span class="p">,</span> <span class="n">Y_test_text</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>Sample of claim text: 1 an audio playback system comprising a playback engine for playing audio data according to control information playback of the audio data a snapshot module comprising a memory for saving a plurality

Predicted title is: party exclusive load sharing an iteration addition in a multi purpose computer . 
 Test title is: music and audio playback system  
---

Sample of claim text: 1 a computer implemented method comprising receiving by a computing system a translation for content from a of the translation the content being associated with an item in an electronic providing the

Predicted title is: necessary internal hybrid point to semiconductor apparatus . 
 Test title is: techniques for translating content  
---

Sample of claim text: 1 a display device for installation in a comprising a detector that detects a touch operation an image associated with the display device that acquires at least two of a navigation image containing ma

Predicted title is: placed microprocessor network sections size . 
 Test title is: display device display program and display method  
---

Sample of claim text: 1 a method of using a computer to determine a of an object in a system comprising receiving from clients in the system the identifying an object detected at the clients determining a of the object on

Predicted title is: stock coverage floating strings monitored applications . 
 Test title is: using confidence about user in a system  
---

Sample of claim text: 1 a system comprising a processor an audio content registry component executable by the processor to register an audio content item wherein the audio content item has an insertion point at which to an

Predicted title is: necessary causing computers space . 
 Test title is: service to audio content with targeted audio advertising to users  
---
</pre></div>


<p>These titles still appear worse than those produced with the Ludwig model, despite a lower loss. The model almost appears to be guessing random words. This may be due to how the decoder is trained.</p>
<hr>
<h2>Summary</h2>
<p>Here we have looked at applying pre-trained word embeddings, and a shared embedding layer, to our sequence to sequence models.</p>
<p>This approach appears to very slightly improve our models. At the very least, it does not appear to worsen the models and provides a useful simplification. Hence, it appears to be a useful addition to our models.</p>
<h3>Making Things Easier</h3>
<p>As we saw in this post, we are now reusing much of our previous code. To simplify our notebooks it would now be a good idea to abstract over some of the functionality. To do this we can create Python objects (classes) that have a common interface for both models, and that maximise reuse of common routines where possible.</p>
<p>To this end we can create four classes:</p>
<ul>
<li>An abstract class that defines the common interfaces. This is found in the file <a href="https://benhoyle.github.io/notebooks/title_generation/abstract_model_wrapper.py">here</a>.</li>
<li>A derived abstract class that defines common sequence-to-sequence model functions. This is found in the file <a href="https://benhoyle.github.io/notebooks/title_generation/base_seq2seq.py">here</a>.</li>
<li>A concrete class for the Ludwig model with shared embeddings. This differs from the above class through the use of a custom <code>_build_model()</code> function. This is found in the file <a href="https://benhoyle.github.io/notebooks/title_generation/ludwig_model.py">here</a>.</li>
<li>A concrete class for the Chollet/Brownlee model with shared embeddings. This also differs from the derived abstract class through the use of a custom <code>_build_model()</code> function. This is found in the file <a href="https://benhoyle.github.io/notebooks/title_generation/cb_model.py">here</a>.</li>
</ul>
<p>We can then concentrate on over-riding the model building part of the class while leaving all other functions the same.</p>
    </div>
    <hr/>
    <footer>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2018-06-21T10:02:55.991555+01:00">
          <i class="fa fa-clock-o"></i>
          Thu 21 June 2018
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="https://benhoyle.github.io/category/title-generation.html">Title Generation</a>
        </li>
          <li class="list-inline-item">
            <i class="fa fa-user-o"></i>
              <a href="https://benhoyle.github.io/author/ben-hoyle.html">Ben Hoyle</a>          </li>
          <li class="list-inline-item">
            <i class="fa fa-files-o"></i>
              <a href="https://benhoyle.github.io/tag/improving_results.html">#improving_results</a>          </li>
      </ul>
    </footer>
  </article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <p class="col-sm-6 text-sm-left">
    <a href="https://www.linkedin.com/in/benhoyle/" class="text-muted" target="_blank">Ben Hoyle</a> - <a href="https://twitter.com/bjh_ip" ><i class="fab fa-twitter"></i></a>
  </p>
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" class="text-muted" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" class="text-muted" target="_blank">Adapted from &#x2728;</a>
  </p>
</div>    </div>
  </footer>
</body>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script>
$( document ).ready(function() {
    $('table').addClass('table table-bordered');
    $('tbody').addClass('table-striped');
    $('th').addClass('text-center');
});
</script></html>